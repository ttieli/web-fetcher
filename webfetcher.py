#!/usr/bin/env python3
"""
Unified web fetcher CLI (single file).
Supports WeChat (mp.weixin.qq.com), Xiaohongshu (xiaohongshu.com), and generic sites.

Features
- Static fetch with UA control; optional headless rendering (Playwright) for JS-heavy pages.
- Site-specific adapters for WeChat and Xiaohongshu; generic fallback.
- Clean Markdown output named as: YYYY-MM-DD - 标题.md
"""

__version__ = "1.0.0"
__author__ = "WebFetcher Team"

import argparse
import datetime
import html as ihtml
import json
import os
import re
import http.client as http_client
import urllib.parse
import urllib.request
import ssl
import subprocess
import sys
from typing import Optional
from html.parser import HTMLParser
from pathlib import Path
import logging
import time
import random
from collections import deque
# BeautifulSoup导入移至动态导入机制

def get_beautifulsoup_parser():
    """动态导入BeautifulSoup，如果不可用则返回None"""
    try:
        from bs4 import BeautifulSoup
        return BeautifulSoup
    except ImportError:
        return None

# HTML解析降级支持
from html.parser import HTMLParser

class FallbackHTMLParser(HTMLParser):
    """基础HTML解析器作为BeautifulSoup的降级方案"""
    
    def __init__(self):
        super().__init__()
        self.title = ""
        self.in_title = False
        self.content_parts = []
        self.current_text = ""
        self.in_script = False
        self.in_style = False
        self.meta_attrs = []
        
    def handle_starttag(self, tag, attrs):
        if tag == 'title':
            self.in_title = True
        elif tag in ['script', 'style']:
            if tag == 'script':
                self.in_script = True
            else:
                self.in_style = True
        elif tag == 'meta':
            attr_dict = dict(attrs)
            self.meta_attrs.append(attr_dict)
        elif tag in ['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'div', 'li']:
            if self.current_text.strip():
                self.content_parts.append(self.current_text.strip())
                self.current_text = ""
                
    def handle_endtag(self, tag):
        if tag == 'title':
            self.in_title = False
        elif tag in ['script', 'style']:
            if tag == 'script':
                self.in_script = False
            else:
                self.in_style = False
        elif tag in ['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'div', 'li']:
            if self.current_text.strip():
                self.content_parts.append(self.current_text.strip())
                self.current_text = ""
                
    def handle_data(self, data):
        if self.in_title:
            self.title += data.strip()
        elif not self.in_script and not self.in_style:
            self.current_text += data
            
    def get_parsed_content(self):
        if self.current_text.strip():
            self.content_parts.append(self.current_text.strip())
        return {
            'title': self.title or '未命名',
            'content_parts': self.content_parts,
            'meta_attrs': self.meta_attrs
        }

def extract_with_htmlparser(html_content: str, url: str) -> tuple[str, str, dict]:
    """使用Python内置HTMLParser进行基础解析"""
    parser = FallbackHTMLParser()
    try:
        parser.feed(html_content)
    except Exception as e:
        logging.warning(f"HTMLParser解析出错: {e}")
        return datetime.datetime.now().strftime('%Y-%m-%d'), f"# 解析失败\n\n无法解析页面内容: {str(e)}", {'page_type': 'parse_error'}
    
    parsed = parser.get_parsed_content()
    
    # 构建markdown内容
    content_parts = [f"# {parsed['title']}\n"]
    
    # 添加meta信息
    if parsed['meta_attrs']:
        content_parts.append("## 页面元数据\n")
        for meta in parsed['meta_attrs']:
            if meta.get('name'):
                content_parts.append(f"- {meta.get('name')}: {meta.get('content', '')}")
            elif meta.get('property'):
                content_parts.append(f"- {meta.get('property')}: {meta.get('content', '')}")
    
    # 添加主要内容
    if parsed['content_parts']:
        content_parts.append("\n## 页面内容\n")
        for part in parsed['content_parts']:
            if part.strip():
                content_parts.append(part.strip() + "\n")
    
    markdown_content = '\n'.join(content_parts)
    date_only = datetime.datetime.now().strftime('%Y-%m-%d')
    
    metadata = {
        'page_type': 'basic_html',
        'parser_used': 'HTMLParser',
        'content_sections': len(parsed['content_parts'])
    }
    
    return date_only, markdown_content, metadata

# Create an SSL context that doesn't verify certificates for legacy sites
ssl_context_unverified = ssl.create_default_context()
ssl_context_unverified.check_hostname = False
ssl_context_unverified.verify_mode = ssl.CERT_NONE

# Multi-page document support constants
MAX_PAGINATION_DEPTH = 5

# Site crawling configuration
MAX_CRAWL_DEPTH = 10  # Absolute maximum to prevent infinite recursion
MAX_CRAWL_PAGES = 1000  # Absolute maximum pages (increased for larger documentation sites)
DEFAULT_CRAWL_DELAY = 0.5  # Polite crawling delay

# Memory protection constants
MAX_PAGE_SIZE = 10 * 1024 * 1024  # 10MB limit for individual pages

# Smart URL filtering constants
BINARY_EXTENSIONS = {'.pdf', '.zip', '.tar', '.gz', '.rar', '.7z', '.exe', '.dmg', '.iso'}
IMAGE_EXTENSIONS = {'.jpg', '.jpeg', '.png', '.gif', '.webp', '.svg', '.ico', '.bmp'}
VIDEO_EXTENSIONS = {'.mp4', '.avi', '.mov', '.wmv', '.flv', '.webm', '.mkv'}
AUDIO_EXTENSIONS = {'.mp3', '.wav', '.flac', '.aac', '.ogg', '.m4a'}
API_PATTERNS = {'/api/', '/rest/', '/graphql', '.json', '.xml', '.rss'}
BUILD_PATTERNS = {'/node_modules/', '/dist/', '/build/', '/.git/', '/target/',
                  '/_next/', '/_nuxt/', '/.next/', '/static/'}

# Retry configuration constants  
MAX_RETRIES = 3
BASE_DELAY = 1.0  # Base delay in seconds (1s, 2s, 4s progression)
MAX_JITTER = 0.1  # Add small random jitter to prevent thundering herd

# Define which exceptions and HTTP status codes are retryable
RETRYABLE_EXCEPTIONS = (
    urllib.error.URLError,           # DNS resolution, connection refused
    http_client.RemoteDisconnected,  # Server closed connection unexpectedly
    http_client.BadStatusLine,       # Malformed HTTP response
    ConnectionResetError,            # Connection reset by peer
    TimeoutError,                    # Socket timeout
    OSError,                        # OS-level network errors
)

RETRYABLE_HTTP_STATUS_CODES = {
    429,  # Too Many Requests (rate limiting)
    500,  # Internal Server Error
    502,  # Bad Gateway
    503,  # Service Unavailable
    504,  # Gateway Timeout
    520, 521, 522, 523, 524,  # CloudFlare errors
}


def setup_logging(verbose: bool = False):
    level = logging.INFO if verbose else logging.WARNING
    logging.basicConfig(
        level=level,
        format='%(asctime)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )


def sanitize_filename(name: str) -> str:
    invalid = set('/\\:*?"<>|\n\r\t')
    name = ''.join(ch if ch not in invalid else ' ' for ch in name)
    name = re.sub(r"\s+", " ", name).strip()
    return name[:160]


def normalize_media_url(u: str) -> str:
    if not u:
        return u
    u = u.strip()
    if u.startswith('//'):
        return 'https:' + u
    return u


def docusaurus_to_markdown(html: str, url: str) -> tuple[str, str, dict]:
    # Detect title: prefer H1 within doc markdown; fallback to og/title
    title = ''
    # H1 inside theme-doc-markdown
    m = re.search(r'<(?:article|div)[^>]+class=["\'][^"\']*theme-doc-markdown[^"\']*["\'][\s\S]*?<h1[^>]*>(.*?)</h1>', html, re.I)
    if m:
        title = ihtml.unescape(re.sub(r'<[^>]+>', '', m.group(1))).strip()
    if not title:
        title = extract_meta(html, 'og:title') or extract_meta(html, 'twitter:title')
    if not title:
        m2 = re.search(r'<title[^>]*>(.*?)</title>', html, re.I|re.S)
        if m2:
            title = ihtml.unescape(re.sub(r'<[^>]+>', '', m2.group(1))).strip()
    title = title or '未命名'

    # Choose a root container: element with class including theme-doc-markdown or markdown
    root_start = None
    for pat in (r'<(?:article|div)[^>]+class=["\'][^"\']*theme-doc-markdown[^"\']*["\']',
                r'<(?:article|div)[^>]+class=["\'][^"\']*\bmarkdown\b[^"\']*["\']'):
        ms = re.search(pat, html, re.I)
        if ms:
            root_start = ms.start()
            break
    # If not found, fallback to <main>
    if root_start is None:
        ms = re.search(r'<main[^>]*>', html, re.I)
        root_start = ms.start() if ms else 0
    html_tail = html[root_start:]

    base_url = url

    class DocParser(HTMLParser):
        def __init__(self):
            super().__init__()
            self.capture = False
            self.depth = 0
            self.parts: list[str] = []
            self.link_stack: list[Optional[str]] = []  # None means no bracket opened
            self.list_stack: list[tuple[bool,int]] = []  # (ordered, next_index)
            self.in_pre = False
            self.code_lang = ''
            self.blockquote_level = 0
            self.in_script = False
            self.images: list[str] = []

        def push_parbreak(self):
            if self.parts and not self.parts[-1].endswith('\n\n'):
                self.parts.append('\n\n')

        def handle_starttag(self, tag, attrs):
            a = dict(attrs)
            cls = a.get('class','') or ''
            if not self.capture and ('theme-doc-markdown' in cls or re.search(r'\bmarkdown\b', cls or '') or tag == 'main'):
                self.capture = True
                self.depth = 1
                return
            if self.capture:
                self.depth += 1
                if tag in ('p','div','section'): self.push_parbreak()
                elif tag == 'br': self.parts.append('\n')
                elif tag == 'h1': self.parts.append('\n\n# ')
                elif tag == 'h2': self.parts.append('\n\n## ')
                elif tag == 'h3': self.parts.append('\n\n### ')
                elif tag == 'h4': self.parts.append('\n\n#### ')
                elif tag == 'h5': self.parts.append('\n\n##### ')
                elif tag == 'h6': self.parts.append('\n\n###### ')
                elif tag == 'blockquote':
                    self.blockquote_level += 1
                    self.parts.append('\n\n' + '> ' * self.blockquote_level)
                elif tag == 'ul':
                    self.list_stack.append((False, 0))
                    self.parts.append('\n')
                elif tag == 'ol':
                    self.list_stack.append((True, 1))
                    self.parts.append('\n')
                elif tag == 'li':
                    indent = '  ' * max(0, len(self.list_stack)-1)
                    if self.list_stack and self.list_stack[-1][0]:
                        ordered, idx = self.list_stack[-1]
                        self.parts.append(f"\n{indent}{idx}. ")
                        self.list_stack[-1] = (ordered, idx+1)
                    else:
                        self.parts.append(f"\n{indent}- ")
                elif tag == 'pre':
                    self.in_pre = True
                    self.code_lang = ''
                    self.parts.append('\n\n```\n')
                elif tag == 'script':
                    self.in_script = True
                elif tag == 'code':
                    if self.in_pre:
                        # language from class like language-python
                        cls = a.get('class','') or ''
                        m = re.search(r'language-([\w+-]+)', cls)
                        if m:
                            # replace opening fence with language
                            if self.parts and self.parts[-1].endswith('```\n'):
                                self.parts[-1] = self.parts[-1][:-4] + m.group(1) + '\n'
                    else:
                        self.parts.append('`')
                elif tag == 'img':
                    src = a.get('src') or a.get('data-src')
                    if src:
                        src = resolve_url_with_context(base_url, src)
                        src = normalize_media_url(src)
                        self.images.append(src)
                        self.parts.append(f"\n\n![]({src})\n\n")
                elif tag == 'a':
                    href = a.get('href') or ''
                    if href and not href.startswith('#'):
                        href = href.strip().split()[0]
                        href = href.split(' target=')[0]
                        href = href.strip('"\'')
                        href = resolve_url_with_context(base_url, href)
                        href = normalize_media_url(href)
                        self.link_stack.append(href)
                        self.parts.append('[')
                    else:
                        self.link_stack.append(None)

        def handle_endtag(self, tag):
            if self.capture:
                if tag == 'a':
                    href = self.link_stack.pop() if self.link_stack else None
                    if href is not None:
                        self.parts.append(f"]({href})")
                elif tag == 'blockquote':
                    self.blockquote_level = max(0, self.blockquote_level-1)
                    self.parts.append('\n\n')
                elif tag == 'pre':
                    self.in_pre = False
                    self.parts.append('\n```\n\n')
                elif tag == 'script':
                    self.in_script = False
                elif tag == 'code':
                    if not self.in_pre:
                        self.parts.append('`')
                elif tag in ('ul','ol'):
                    if self.list_stack:
                        self.list_stack.pop()
                        self.parts.append('\n')
                self.depth -= 1
                if self.depth == 0:
                    self.capture = False

        def handle_data(self, data):
            if self.capture and not self.in_script:
                if self.in_pre:
                    self.parts.append(data)
                else:
                    t = data.replace('\r','')
                    if t.strip():
                        if t.strip() == '¶':
                            return
                        if self.blockquote_level:
                            # indent blockquote lines
                            pref = '> ' * self.blockquote_level
                            lines = [pref + ihtml.unescape(x) for x in t.splitlines() if x.strip()]
                            self.parts.append('\n'.join(lines))
                        else:
                            self.parts.append(ihtml.unescape(t))

    parser = DocParser()
    parser.feed(html_tail)
    body = ''.join(parser.parts)
    body = re.sub(r'\n{3,}', '\n\n', body).strip()
    date_only, date_time = parse_date_like(extract_meta(html, 'article:published_time') or extract_meta(html, 'og:updated_time'))
    desc = extract_meta(html, 'description')
    lines = [f"# {title}", f"- 标题: {title}", f"- 发布时间: {date_time}", f"- 来源: [{url}]({url})", f"- 抓取时间: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"]
    if body:
        lines += ["", body]
    else:
        lines += ["", '(未能提取正文)']
    
    metadata = {
        'description': desc,
        'images': parser.images,
        'publish_time': extract_meta(html, 'article:published_time') or extract_meta(html, 'og:updated_time')
    }
    return date_only, "\n\n".join(lines).strip() + "\n", metadata


def mkdocs_to_markdown(html: str, url: str) -> tuple[str, str, dict]:
    # Title from h1 within md-typeset, else <title>
    title = ''
    m = re.search(r'<article[^>]+class=["\'][^"\']*md-content__inner[^"\']*["\'][\s\S]*?<h1[^>]*>(.*?)</h1>', html, re.I)
    if m:
        title = ihtml.unescape(re.sub(r'<[^>]+>', '', m.group(1))).strip()
    if not title:
        m2 = re.search(r'<title[^>]*>(.*?)</title>', html, re.I|re.S)
        if m2:
            title = ihtml.unescape(re.sub(r'<[^>]+>', '', m2.group(1))).strip()
    title = title or '未命名'

    # Use the article md-typeset region as root
    ms = re.search(r'<article[^>]+class=["\'][^"\']*md-content__inner[^"\']*["\']', html, re.I)
    root_start = ms.start() if ms else 0
    html_tail = html[root_start:]

    base_url = url

    class MkParser(HTMLParser):
        def __init__(self):
            super().__init__()
            self.capture = False
            self.depth = 0
            self.parts: list[str] = []
            self.link_stack: list[Optional[str]] = []
            self.list_stack: list[tuple[bool,int]] = []
            self.in_pre = False
            self.blockquote_level = 0
            self.in_script = False
            self.images: list[str] = []

        def handle_starttag(self, tag, attrs):
            a = dict(attrs)
            cls = a.get('class','') or ''
            if not self.capture and ('md-content__inner' in cls or tag == 'article'):
                self.capture = True
                self.depth = 1
                return
            if self.capture:
                self.depth += 1
                if tag in ('p','div','section'): self.parts.append('\n\n')
                elif tag == 'br': self.parts.append('\n')
                elif tag == 'h1': self.parts.append('\n\n# ')
                elif tag == 'h2': self.parts.append('\n\n## ')
                elif tag == 'h3': self.parts.append('\n\n### ')
                elif tag == 'h4': self.parts.append('\n\n#### ')
                elif tag == 'h5': self.parts.append('\n\n##### ')
                elif tag == 'h6': self.parts.append('\n\n###### ')
                elif tag == 'blockquote':
                    self.blockquote_level += 1
                    self.parts.append('\n\n' + '> ' * self.blockquote_level)
                elif tag == 'ul':
                    self.list_stack.append((False, 0))
                    self.parts.append('\n')
                elif tag == 'ol':
                    self.list_stack.append((True, 1))
                    self.parts.append('\n')
                elif tag == 'li':
                    indent = '  ' * max(0, len(self.list_stack)-1)
                    if self.list_stack and self.list_stack[-1][0]:
                        ordered, idx = self.list_stack[-1]
                        self.parts.append(f"\n{indent}{idx}. ")
                        self.list_stack[-1] = (ordered, idx+1)
                    else:
                        self.parts.append(f"\n{indent}- ")
                elif tag == 'pre':
                    self.in_pre = True
                    self.parts.append('\n\n```\n')
                elif tag == 'script':
                    self.in_script = True
                elif tag == 'code':
                    if not self.in_pre:
                        self.parts.append('`')
                elif tag == 'img':
                    src = a.get('src') or a.get('data-src')
                    if src:
                        src = resolve_url_with_context(base_url, src)
                        src = normalize_media_url(src)
                        self.images.append(src)
                        self.parts.append(f"\n\n![]({src})\n\n")
                elif tag == 'a':
                    href = a.get('href') or ''
                    if href and not href.startswith('#'):
                        href = href.strip().split()[0]
                        href = href.split(' target=')[0]
                        href = href.strip('"\'')
                        href = resolve_url_with_context(base_url, href)
                        href = normalize_media_url(href)
                        self.link_stack.append(href)
                        self.parts.append('[')
                    else:
                        self.link_stack.append(None)

        def handle_endtag(self, tag):
            if self.capture:
                if tag == 'a':
                    href = self.link_stack.pop() if self.link_stack else None
                    if href is not None:
                        self.parts.append(f"]({href})")
                elif tag == 'blockquote':
                    self.blockquote_level = max(0, self.blockquote_level-1)
                    self.parts.append('\n\n')
                elif tag == 'pre':
                    self.in_pre = False
                    self.parts.append('\n```\n\n')
                elif tag == 'script':
                    self.in_script = False
                elif tag == 'code':
                    if not self.in_pre:
                        self.parts.append('`')
                elif tag in ('ul','ol'):
                    if self.list_stack:
                        self.list_stack.pop()
                        self.parts.append('\n')
                self.depth -= 1
                if self.depth == 0:
                    self.capture = False

        def handle_data(self, data):
            if self.capture and not self.in_script:
                if self.in_pre:
                    self.parts.append(data)
                else:
                    t = data.replace('\r','')
                    if t.strip():
                        if t.strip() == '¶':
                            return
                        self.parts.append(ihtml.unescape(t))

    parser = MkParser()
    parser.feed(html_tail)
    body = ''.join(parser.parts)
    body = re.sub(r'\n{3,}', '\n\n', body).strip()
    date_only, date_time = parse_date_like(extract_meta(html, 'article:published_time') or extract_meta(html, 'og:updated_time'))
    desc = extract_meta(html, 'description')
    lines = [f"# {title}", f"- 标题: {title}", f"- 发布时间: {date_time}", f"- 来源: [{url}]({url})", f"- 抓取时间: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"]
    lines += ["", body or '(未能提取正文)']
    
    metadata = {
        'description': desc,
        'images': parser.images,
        'publish_time': extract_meta(html, 'article:published_time') or extract_meta(html, 'og:updated_time')
    }
    return date_only, "\n\n".join(lines).strip() + "\n", metadata


def should_retry_exception(exc: Exception) -> bool:
    """Determine if an exception warrants a retry attempt."""
    if isinstance(exc, urllib.error.HTTPError):
        return exc.status in RETRYABLE_HTTP_STATUS_CODES
    return isinstance(exc, RETRYABLE_EXCEPTIONS)

def calculate_backoff_delay(attempt: int, base_delay: float = BASE_DELAY) -> float:
    """Calculate exponential backoff delay with jitter."""
    delay = base_delay * (2 ** attempt)  # 1s, 2s, 4s
    jitter = random.uniform(0, MAX_JITTER)
    return delay + jitter

def fetch_html_with_retry(url: str, ua: Optional[str] = None, timeout: int = 30) -> str:
    """
    Fetch HTML with exponential backoff retry logic.
    
    Retries network/temporary errors up to MAX_RETRIES times with exponential backoff.
    Immediately fails on client errors (4xx) and non-retryable server errors.
    """
    last_exception = None
    
    for attempt in range(MAX_RETRIES + 1):  # 0, 1, 2, 3 (4 total attempts)
        try:
            if attempt > 0:
                delay = calculate_backoff_delay(attempt - 1)
                logging.info(f"Retry attempt {attempt}/{MAX_RETRIES} for {url} after {delay:.1f}s delay")
                time.sleep(delay)
            
            # Call the original fetch_html function
            return fetch_html_original(url, ua, timeout)
            
        except Exception as e:
            last_exception = e
            
            # Log the error with context
            if attempt == 0:
                logging.warning(f"Initial fetch failed for {url}: {type(e).__name__}: {e}")
            else:
                logging.warning(f"Retry {attempt}/{MAX_RETRIES} failed for {url}: {type(e).__name__}: {e}")
            
            # Check if we should retry this exception
            if not should_retry_exception(e):
                # Special handling for HTTP 307 redirect loops
                if isinstance(e, urllib.error.HTTPError) and e.status == 307:
                    logging.error(f"HTTP 307 redirect loop detected for {url}. "
                                 f"This may indicate a redirect loop. "
                                 f"Try using a specific page URL instead of the root domain.")
                else:
                    logging.info(f"Non-retryable error for {url}, failing immediately: {type(e).__name__}")
                raise e
            
            # If this was the last attempt, don't sleep
            if attempt == MAX_RETRIES:
                break
    
    # All retry attempts exhausted
    logging.error(f"All {MAX_RETRIES + 1} attempts failed for {url}, giving up")
    raise last_exception

def fetch_html_with_curl(url: str, ua: Optional[str] = None, timeout: int = 30) -> str:
    """Fallback to curl for sites with SSL issues"""
    ua = ua or "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0 Safari/537.36"
    try:
        cmd = [
            'curl', '-k', '-s', '-L',  # -k ignores SSL, -s silent, -L follow redirects
            '--max-time', str(timeout),
            '-H', f'User-Agent: {ua}',
            '-H', 'Accept-Language: zh-CN,zh;q=0.9',
            '--compressed',  # Accept compressed responses
            url
        ]
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=timeout+5)
        if result.returncode == 0:
            return result.stdout
        else:
            raise Exception(f"curl failed with code {result.returncode}: {result.stderr}")
    except subprocess.TimeoutExpired:
        raise Exception(f"curl timeout for {url}")
    except Exception as e:
        logging.error(f"Failed to fetch with curl from {url}: {e}")
        raise

def fetch_html_original(url: str, ua: Optional[str] = None, timeout: int = 30) -> str:
    ua = ua or "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0 Safari/537.36"
    req = urllib.request.Request(url, headers={"User-Agent": ua, "Accept-Language": "zh-CN,zh;q=0.9"})
    try:
        # Use unverified SSL context for sites with legacy SSL configurations
        with urllib.request.urlopen(req, timeout=timeout, context=ssl_context_unverified) as r:
            try:
                data = r.read(MAX_PAGE_SIZE)  # Limit read size
                # Check if there's more data and truncate if needed
                remaining = r.read(1)
                if remaining:
                    logging.warning(f"Page truncated at {MAX_PAGE_SIZE} bytes: {url}")
            except http_client.IncompleteRead as e:
                logging.warning(f"Incomplete read, using partial data: {len(e.partial or b'')} bytes")
                data = (e.partial or b"")
        return data.decode("utf-8", errors="ignore")
    except Exception as e:
        # If SSL error, try curl as fallback
        if "SSL" in str(e) or "CERTIFICATE" in str(e).upper():
            logging.info(f"SSL error detected, falling back to curl for {url}")
            return fetch_html_with_curl(url, ua, timeout)
        logging.error(f"Failed to fetch HTML from {url}: {e}")
        raise

# Replace the public interface to use the retry wrapper
fetch_html = fetch_html_with_retry


def try_render(url: str, ua: Optional[str] = None, timeout_ms: int = 60000) -> Optional[str]:
    try:
        from playwright.sync_api import sync_playwright
    except Exception:
        return None
    html = None
    try:
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=True, args=['--no-sandbox','--disable-blink-features=AutomationControlled'])
            ctx = browser.new_context(user_agent=ua or 'Mozilla/5.0 (iPhone; CPU iPhone OS 16_5 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148', locale='zh-CN', viewport={'width':390,'height':844}, device_scale_factor=3)
            page = ctx.new_page()
            page.set_extra_http_headers({'Accept-Language':'zh-CN,zh;q=0.9'})
            # Relaxed waiting strategy to avoid networkidle stalls
            page.goto(url, wait_until='domcontentloaded', timeout=timeout_ms)
            page.wait_for_load_state('load', timeout=timeout_ms)
            page.wait_for_timeout(800)
            try:
                page.evaluate('window.scrollTo(0, document.body.scrollHeight)')
                page.wait_for_timeout(600)
            except Exception:
                pass
            html = page.content()
            ctx.close(); browser.close()
    except Exception:
        html = None
    return html


def extract_meta(html: str, name_or_prop: str) -> str:
    m = re.search(rf'<meta[^>]+(?:name|property)=["\']{re.escape(name_or_prop)}["\'][^>]+content=["\']([^"\']*)["\']', html, re.I)
    return ihtml.unescape(m.group(1).strip()) if m else ""


def extract_json_ld_content(html: str) -> dict:
    """Extract content from JSON-LD structured data."""
    import json
    
    result = {
        'description': '',
        'articleBody': '',
        'datePublished': '',
        'dateModified': '',
        'author': ''
    }
    
    # Find all JSON-LD scripts
    pattern = r'<script[^>]+type=["\']application/ld\+json["\'][^>]*>(.*?)</script>'
    matches = re.findall(pattern, html, re.I | re.S)
    
    for match in matches:
        try:
            data = json.loads(match.strip())
            
            # Handle both single object and @graph array
            items = data.get('@graph', [data]) if isinstance(data, dict) else [data]
            
            for item in items:
                if not isinstance(item, dict):
                    continue
                    
                item_type = item.get('@type', '')
                
                # Check for Article-like types
                if any(t in str(item_type) for t in ['Article', 'NewsArticle', 'BlogPosting']):
                    result['description'] = item.get('description', result['description'])
                    result['articleBody'] = item.get('articleBody', item.get('text', result['articleBody']))
                    result['datePublished'] = item.get('datePublished', result['datePublished'])
                    result['dateModified'] = item.get('dateModified', result['dateModified'])
                    
                    # Extract author
                    author = item.get('author', {})
                    if isinstance(author, dict):
                        result['author'] = author.get('name', '')
                    elif isinstance(author, str):
                        result['author'] = author
                
                # Also check Person/Organization for government sites
                elif 'Person' in str(item_type) or 'Organization' in str(item_type):
                    if not result['description']:
                        result['description'] = item.get('description', '')
        except (json.JSONDecodeError, AttributeError):
            continue
    
    return result


def parse_date_like(s: Optional[str]) -> tuple[str, str]:
    if not s:
        now = datetime.datetime.now()
        return now.strftime("%Y-%m-%d"), now.strftime("%Y-%m-%d %H:%M:%S")
    s = str(s)
    m = re.match(r"^(\d{10,})(?::\d{2})?$", s)
    dt = None
    if m:
        num = int(m.group(1))
        dt = datetime.datetime.fromtimestamp(num/1000 if num > 10_000_000_000 else num)
    if dt is None:
        s2 = s.replace('年','-').replace('月','-').replace('日','').replace('/','-')
        m2 = re.search(r'(20\d{2})-([01]?\d)-([0-3]?\d)', s2)
        if m2:
            y, mo, d = m2.groups()
            dt = datetime.datetime(int(y), int(mo), int(d))
    if dt is None:
        now = datetime.datetime.now()
        return now.strftime("%Y-%m-%d"), now.strftime("%Y-%m-%d %H:%M:%S")
    return dt.strftime("%Y-%m-%d"), dt.strftime("%Y-%m-%d %H:%M:%S")


def ensure_unique_path(outdir: Path, base: str) -> Path:
    p = outdir / f"{base}.md"
    if not p.exists():
        return p
    n = 2
    while True:
        cand = outdir / f"{base} ({n}).md"
        if not cand.exists():
            return cand
        n += 1


def wechat_to_markdown(html: str, url: str) -> tuple[str, str, dict]:
    title = extract_meta(html, 'og:title')
    if not title:
        m = re.search(r'<h1[^>]*class=["\'][^"\']*rich_media_title[^"\']*["\'][^>]*>(.*?)</h1>', html, re.I|re.S)
        if m:
            t = re.sub(r'<[^>]+>', '', m.group(1))
            title = ihtml.unescape(t).strip()
    if not title:
        title = '未命名'

    author = extract_meta(html, 'og:article:author')
    if not author:
        m = re.search(r'<span[^>]*class=["\'][^"\']*rich_media_meta\s+rich_media_meta_text[^"\']*["\'][^>]*>(.*?)</span>', html, re.I|re.S)
        if m:
            author = ihtml.unescape(re.sub(r'<[^>]+>', '', m.group(1))).strip()

    pub = ''
    for pat in [r'id=["\']publish_time["\'][^>]*>([^<]+)<', r'property=["\']article:published_time["\'][^>]+content=["\']([^"\']+)["\']']:
        m = re.search(pat, html, re.I)
        if m:
            pub = ihtml.unescape(m.group(1).strip())
            break
    date_only, date_time = parse_date_like(pub)

    class WxParser(HTMLParser):
        def __init__(self):
            super().__init__()
            self.capture = False
            self.depth = 0
            self.parts: list[str] = []
            self.link = None
            self.images: list[str] = []
        def handle_starttag(self, tag, attrs):
            a = dict(attrs)
            if not self.capture and a.get('id') == 'js_content':
                self.capture = True
                self.depth = 1
                return
            if self.capture:
                self.depth += 1
                if tag in ('p','div','section'): self.parts.append('\n\n')
                elif tag in ('br','hr'): self.parts.append('\n')
                elif tag == 'li': self.parts.append('\n- ')
                elif tag == 'h1': self.parts.append('\n\n# ')
                elif tag == 'h2': self.parts.append('\n\n## ')
                elif tag == 'h3': self.parts.append('\n\n### ')
                elif tag == 'img':
                    src = a.get('data-src') or a.get('src')
                    if src:
                        src = normalize_media_url(src)
                        self.images.append(src)
                        self.parts.append(f"\n\n![]({src})\n\n")
                elif tag == 'a':
                    self.link = a.get('href')
        def handle_endtag(self, tag):
            if self.capture:
                if tag == 'a' and self.link:
                    self.parts.append(f" ({self.link})")
                    self.link = None
                self.depth -= 1
                if self.depth == 0:
                    self.capture = False
        def handle_data(self, data):
            if self.capture:
                t = data.strip('\n')
                if t.strip(): self.parts.append(ihtml.unescape(t))

    p = WxParser()
    p.feed(html)
    body = ''.join(p.parts)
    body = re.sub(r'\n{3,}', '\n\n', body).strip() or '(未能提取正文)'

    lines = [f"# {title}"]
    meta = [f"- 标题: {title}"]
    if author: meta.append(f"- 作者: {author}")
    meta += [f"- 发布时间: {date_time}", f"- 来源: [{url}]({url})", f"- 抓取时间: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"]
    lines += meta + ["", body]
    
    metadata = {
        'author': author,
        'images': p.images,
        'publish_time': pub
    }
    return date_only, "\n\n".join(lines).strip() + "\n", metadata


def xhs_to_markdown(html: str, url: str) -> tuple[str, str, dict]:
    def clean_title(t: str) -> str:
        t = t.strip()
        t = re.sub(r"\s*-\s*小红书\s*$", "", t)
        return t
    # title
    title = clean_title(extract_meta(html, 'og:title') or extract_meta(html, 'twitter:title') or '')
    if not title:
        m = re.search(r'<title[^>]*>(.*?)</title>', html, re.I|re.S)
        if m:
            title = clean_title(ihtml.unescape(re.sub(r'<[^>]+>', '', m.group(1)))).strip()
    title = title or '未命名'
    # author/date from JSON-LD
    author = ''
    date_raw = ''
    for m in re.finditer(r'<script[^>]+type=["\']application/ld\+json["\'][^>]*>(.*?)</script>', html, re.I|re.S):
        txt = m.group(1).strip()
        try:
            obj = json.loads(txt)
        except Exception:
            continue
        def visit(o):
            nonlocal author, date_raw
            if not isinstance(o, dict):
                return
            if not author:
                a = o.get('author') or {}
                if isinstance(a, dict):
                    nm = (a.get('name') or '').strip()
                    if nm and nm.lower() != 'undefined':
                        author = nm
            if not date_raw:
                for k in ('datePublished','uploadDate'):
                    v = o.get(k)
                    if v and re.search(r"\d{6,}|20\d{2}", str(v)):
                        date_raw = str(v)
                        break
        if isinstance(obj, list):
            for it in obj: visit(it)
        elif isinstance(obj, dict):
            visit(obj)
            if isinstance(obj.get('@graph'), list):
                for it in obj['@graph']: visit(it)
    # fallback date scan
    if not date_raw:
        m = re.search(r'"(datePublished|uploadDate)"\s*:\s*"([^"]+)"', html)
        if m: date_raw = m.group(2)
    date_only, date_time = parse_date_like(date_raw)

    desc = extract_meta(html, 'description').replace('\t','\n\n').strip()
    cover = extract_meta(html, 'og:image')
    # image gallery - collect from attributes and JSON strings
    imgs: list[str] = []
    seen = set()
    def consider(u: str):
        if not u:
            return
        # strip quotes and spaces
        u2 = u.strip().strip('"\'')
        # heuristic filters for XHS media images (exclude avatars/icons)
        ok_domain = any(x in u2 for x in (
            'ci.xiaohongshu.com',
            'sns-img',
            'xhscdn.com',
        ))
        if not ok_domain:
            return
        if any(bad in u2 for bad in ('avatar','favicon')):
            return
        # must look like an image URL (extension or image processing params)
        if not (re.search(r'\.(?:jpg|jpeg|png|webp|gif)(?:\?|$)', u2, re.I) or ('imageMogr2' in u2) or ('imageView2' in u2)):
            return
        if u2 not in seen:
            seen.add(u2)
            imgs.append(u2)

    # 1) common attributes: src, data-src, srcset
    for m in re.finditer(r'(?:src|data-src)=["\']([^"\']+)["\']', html, re.I):
        consider(m.group(1))
    # srcset can contain multiple URLs
    for m in re.finditer(r'srcset=["\']([^"\']+)["\']', html, re.I):
        chunk = m.group(1)
        for part in chunk.split(','):
            consider(part.strip().split(' ')[0])
    # 2) generic URLs inside scripts/JSON
    for m in re.finditer(r'"(https?://[^"\s]+\.(?:jpg|jpeg|png|webp)(?:\?[^"\s]*)?)"', html, re.I):
        consider(m.group(1))
    # Ensure cover first
    if cover:
        consider(cover)
        # move cover to front if present later
        if imgs and imgs[0] != cover and cover in imgs:
            imgs.remove(cover)
            imgs.insert(0, cover)

    lines = [f"# {title}"]
    meta = [f"- 标题: {title}"]
    if author: meta.append(f"- 作者: {author}")
    meta += [f"- 发布时间: {date_time}", f"- 来源: {url}", f"- 抓取时间: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"]
    lines += meta
    if cover:
        lines += ["", f"![]({normalize_media_url(cover)})"]
    body = desc or '(未能从页面提取正文摘要)'
    lines += ["", body]
    if imgs:
        lines += ["", "## 图片", ""] + [f"![]({normalize_media_url(u)})" for u in imgs]
    
    metadata = {
        'author': author,
        'images': [normalize_media_url(u) for u in imgs],
        'cover': normalize_media_url(cover) if cover else '',
        'description': desc,
        'publish_time': date_raw
    }
    return date_only, "\n\n".join(lines).strip() + "\n", metadata

def dianping_to_markdown(html: str, url: str) -> tuple[str, str, dict]:
    def clean_title(t: str) -> str:
        t = t.strip()
        t = re.sub(r"\s*[-|]\s*大众点评\s*$", "", t)
        return t

    # Prefer page/og/twitter titles
    title = clean_title(extract_meta(html, 'og:title') or extract_meta(html, 'twitter:title') or '')
    if not title:
        m = re.search(r'<title[^>]*>(.*?)</title>', html, re.I | re.S)
        if m:
            title = clean_title(ihtml.unescape(re.sub(r'<[^>]+>', '', m.group(1)))).strip()
    title = title or '未命名'

    # Try to parse JSON-LD for LocalBusiness-like info
    biz = {}
    for m in re.finditer(r'<script[^>]+type=["\']application/ld\+json["\'][^>]*>(.*?)</script>', html, re.I | re.S):
        txt = m.group(1).strip()
        try:
            obj = json.loads(txt)
        except Exception:
            continue
        cand_list = obj if isinstance(obj, list) else [obj]
        for o in cand_list:
            if not isinstance(o, dict):
                continue
            tp = o.get('@type')
            # Accept LocalBusiness / Restaurant / FoodEstablishment etc.
            if (isinstance(tp, str) and re.search(r'LocalBusiness|Restaurant|FoodEstablishment', tp, re.I)) or (
                isinstance(tp, list) and any(re.search(r'LocalBusiness|Restaurant|FoodEstablishment', str(t), re.I) for t in tp)
            ):
                biz = o
                break
        if biz:
            break

    # Extract fields
    name = (biz.get('name') if isinstance(biz, dict) else '') or title
    telephone = biz.get('telephone') if isinstance(biz, dict) else ''
    price_range = biz.get('priceRange') if isinstance(biz, dict) else ''
    rating = ''
    review_count = ''
    if isinstance(biz, dict) and isinstance(biz.get('aggregateRating'), dict):
        rating = str(biz['aggregateRating'].get('ratingValue') or '')
        review_count = str(biz['aggregateRating'].get('reviewCount') or '')
    # Address may be string or structured
    address = ''
    if isinstance(biz, dict):
        addr = biz.get('address')
        if isinstance(addr, str):
            address = addr
        elif isinstance(addr, dict):
            parts = [addr.get('addressRegion'), addr.get('addressLocality'), addr.get('streetAddress')]
            address = ' '.join([p for p in parts if p])

    # Opening hours
    hours = ''
    if isinstance(biz, dict):
        oh = biz.get('openingHours')
        if isinstance(oh, list):
            hours = '; '.join([str(x) for x in oh if x])
        elif isinstance(oh, str):
            hours = oh
        elif isinstance(biz.get('openingHoursSpecification'), list):
            chunks = []
            for spec in biz['openingHoursSpecification']:
                if not isinstance(spec, dict):
                    continue
                day = spec.get('dayOfWeek')
                opens = spec.get('opens')
                closes = spec.get('closes')
                seg = ' '.join([str(day or '').strip(), f"{opens or ''}-{closes or ''}" ]).strip()
                if seg:
                    chunks.append(seg)
            hours = '; '.join(chunks)

    # Description
    desc = extract_meta(html, 'description').strip()

    # Publish time is not meaningful for shops; use now
    date_only, date_time = parse_date_like(None)

    # Images: collect likely shop images from HTML and script JSONs
    cover = extract_meta(html, 'og:image')
    imgs: list[str] = []
    seen = set()
    def consider(u: str):
        if not u:
            return
        u2 = u.strip().strip('"\'')
        ok_domain = any(x in u2 for x in (
            'dpfile.com',   # Dianping CDN
            'meituan.net',  # Meituan CDN variants p0/p1...
            'dianping.com', # fallback
        ))
        if not ok_domain:
            return
        if any(bad in u2 for bad in ('avatar', 'favicon', 'icon')):
            return
        if not re.search(r'\.(?:jpg|jpeg|png|webp|gif)(?:\?|$)', u2, re.I):
            return
        if u2 not in seen:
            seen.add(u2)
            imgs.append(u2)

    for m in re.finditer(r'(?:src|data-src)=["\']([^"\']+)["\']', html, re.I):
        consider(m.group(1))
    for m in re.finditer(r'srcset=["\']([^"\']+)["\']', html, re.I):
        chunk = m.group(1)
        for part in chunk.split(','):
            consider(part.strip().split(' ')[0])
    for m in re.finditer(r'"(https?://[^"\s]+\.(?:jpg|jpeg|png|webp|gif)(?:\?[^"\s]*)?)"', html, re.I):
        consider(m.group(1))
    if cover:
        consider(cover)
        if imgs and imgs[0] != cover and cover in imgs:
            imgs.remove(cover)
            imgs.insert(0, cover)

    # Compose markdown
    lines = [f"# {name}"]
    meta = [f"- 标题: {name}"]
    if rating:
        meta.append(f"- 评分: {rating}{(' / ' + review_count + '条') if review_count else ''}")
    if price_range:
        meta.append(f"- 人均/价位: {price_range}")
    if telephone:
        meta.append(f"- 电话: {telephone}")
    if address:
        meta.append(f"- 地址: {address}")
    if hours:
        meta.append(f"- 营业时间: {hours}")
    meta += [f"- 发布时间: {date_time}", f"- 来源: [{url}]({url})", f"- 抓取时间: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"]
    lines += meta
    if cover:
        lines += ["", f"![]({normalize_media_url(cover)})"]
    body = desc or '（暂无描述，已抓取基础信息）'
    lines += ["", body]
    if imgs:
        lines += ["", "## 图片", ""] + [f"![]({normalize_media_url(u)})" for u in imgs]
    
    metadata = {
        'rating': rating,
        'review_count': review_count,
        'price_range': price_range,
        'telephone': telephone,
        'address': address,
        'hours': hours,
        'images': [normalize_media_url(u) for u in imgs],
        'cover': normalize_media_url(cover) if cover else '',
        'description': desc
    }
    return date_only, "\n\n".join(lines).strip() + "\n", metadata


def ebchina_news_list_to_markdown(html: str, url: str) -> tuple[str, str, dict]:
    """Parse EB China news list page."""
    # Extract page title
    title_match = re.search(r'<title[^>]*>(.*?)</title>', html, re.I|re.S)
    title = ihtml.unescape(re.sub(r'<[^>]+>', '', title_match.group(1))).strip() if title_match else '新闻列表'
    
    # Extract news items
    news_items = []
    
    # Pattern to extract news items with title, summary, date, and link
    # More specific pattern to correctly capture the title attribute
    item_pattern = r'<p class="N_title"[^>]*><a href="([^"]+)"[^>]*\stitle="([^"]+)"[^>]*>.*?</a></p>.*?<p class="N_summary">(.*?)</p>.*?<p class="N_date">发布时间：([^<]+)</p>'
    
    items = re.findall(item_pattern, html, re.I|re.S)
    
    for link, item_title, summary, date in items:
        # Clean summary
        summary = re.sub(r'<[^>]+>', '', summary)
        summary = ihtml.unescape(summary).strip()
        
        # Clean title
        item_title = ihtml.unescape(item_title).strip()
        
        # Make full URL
        full_url = resolve_url_with_context(url, link)
        
        news_items.append({
            'title': item_title,
            'summary': summary,
            'date': date.strip(),
            'url': full_url
        })
    
    # Build markdown content
    lines = [f"# {title}", ""]
    lines.append(f"- 来源: [{url}]({url})")
    lines.append(f"- 抓取时间: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    lines.append(f"- 新闻数量: {len(news_items)} 条")
    lines.append("")
    lines.append("---")
    lines.append("")
    
    # Add news items
    for item in news_items:
        lines.append(f"## [{item['title']}]({item['url']})")
        lines.append("")
        lines.append(f"**发布时间:** {item['date']}")
        lines.append("")
        lines.append(item['summary'])
        lines.append("")
        lines.append("---")
        lines.append("")
    
    metadata = {
        'news_count': len(news_items),
        'news_items': news_items,
        'page_type': 'news_list'
    }
    
    date_only = datetime.datetime.now().strftime('%Y-%m-%d')
    return date_only, "\n".join(lines).strip() + "\n", metadata


def raw_to_markdown(html: str, url: str) -> tuple[str, str, dict]:
    """
    Raw mode parser - 尽可能保留所有内容的解析器
    
    设计原则：
    1. 宁可冗余，不可遗漏
    2. 保持原始结构
    3. 最小化处理
    4. 明确标记不同类型的内容
    """
    
    # 智能解析：优先使用BeautifulSoup，降级到HTMLParser
    BeautifulSoup = get_beautifulsoup_parser()
    
    if BeautifulSoup:
        # 使用BeautifulSoup进行解析（更宽容的解析器）
        soup = BeautifulSoup(html, 'html.parser')
        parser_used = "BeautifulSoup"
    else:
        # 降级到HTMLParser方案
        logging.warning("BeautifulSoup不可用，使用HTMLParser降级解析。建议安装: pip install beautifulsoup4")
        return extract_with_htmlparser(html, url)
    
    # 1. 提取基本元数据
    title = soup.title.string if soup.title else '未命名'
    
    # 2. 移除不需要的元素（但保守处理）
    for tag in soup(['script', 'style', 'noscript']):
        tag.decompose()
    
    # 3. 构建内容列表
    content_parts = []
    
    # 3.1 保留所有meta信息
    meta_section = ["## 页面元数据\n"]
    for meta in soup.find_all('meta'):
        if meta.get('name'):
            meta_section.append(f"- {meta.get('name')}: {meta.get('content', '')}")
        elif meta.get('property'):
            meta_section.append(f"- {meta.get('property')}: {meta.get('content', '')}")
    
    if len(meta_section) > 1:
        content_parts.append('\n'.join(meta_section))
    
    # 3.2 提取主体内容（保持结构）
    content_parts.append("\n## 页面内容\n")
    
    def extract_text_with_structure(element, level=0):
        """递归提取文本，保持结构"""
        output = []
        
        if element.name:
            # 处理标题
            if element.name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:
                h_level = int(element.name[1])
                output.append('#' * h_level + ' ' + element.get_text(strip=True))
            
            # 处理段落
            elif element.name == 'p':
                text = element.get_text(strip=True)
                if text:  # 即使很短也保留
                    output.append(text)
            
            # 处理列表
            elif element.name in ['ul', 'ol']:
                for li in element.find_all('li', recursive=False):
                    output.append('- ' + li.get_text(strip=True))
            
            # 处理表格（简单ASCII表示）
            elif element.name == 'table':
                output.append("\n[表格内容]")
                for row in element.find_all('tr'):
                    cells = row.find_all(['td', 'th'])
                    if cells:
                        row_text = ' | '.join(cell.get_text(strip=True) for cell in cells)
                        output.append('| ' + row_text + ' |')
            
            # 处理图片
            elif element.name == 'img':
                alt = element.get('alt', '')
                src = element.get('src', '')
                output.append(f"[图片: {alt or src}]")
            
            # 处理视频
            elif element.name == 'video':
                src = element.get('src', '多个源')
                output.append(f"[视频: {src}]")
            
            # 处理音频
            elif element.name == 'audio':
                src = element.get('src', '多个源')
                output.append(f"[音频: {src}]")
            
            # 处理链接（保留链接文本和URL）
            elif element.name == 'a':
                href = element.get('href', '')
                text = element.get_text(strip=True)
                if text and href:
                    output.append(f"[{text}]({href})")
                elif text:
                    output.append(text)
            
            # 处理块引用
            elif element.name == 'blockquote':
                text = element.get_text(strip=True)
                if text:
                    output.append('> ' + text)
            
            # 处理预格式化文本
            elif element.name in ['pre', 'code']:
                text = element.get_text(strip=False)  # 保留空白
                if text:
                    output.append('```\n' + text + '\n```')
            
            # 其他块级元素
            elif element.name in ['div', 'section', 'article', 'main', 'aside', 'header', 'footer']:
                # 递归处理子元素
                for child in element.children:
                    if hasattr(child, 'name'):
                        child_output = extract_text_with_structure(child, level + 1)
                        output.extend(child_output)
                    elif isinstance(child, str):
                        text = child.strip()
                        if text and len(text) > 1:  # 保留几乎所有文本
                            output.append(text)
            
            # 处理其他内联元素
            elif element.name in ['span', 'strong', 'em', 'b', 'i', 'u']:
                text = element.get_text(strip=True)
                if text:
                    output.append(text)
        
        return output
    
    # 提取body内容
    body = soup.body if soup.body else soup
    body_content = []
    
    # 直接处理body的子元素
    for child in body.children:
        if hasattr(child, 'name'):
            child_output = extract_text_with_structure(child, 0)
            body_content.extend(child_output)
        elif isinstance(child, str):
            text = child.strip()
            if text and len(text) > 1:  # 保留几乎所有文本
                body_content.append(text)
    
    # 去除连续空行，但保留段落结构
    cleaned_content = []
    prev_empty = False
    for line in body_content:
        if not line:
            if not prev_empty:
                cleaned_content.append('')
                prev_empty = True
        else:
            cleaned_content.append(line)
            prev_empty = False
    
    content_parts.extend(cleaned_content)
    
    # 3.3 提取所有链接（作为附录）
    all_links = []
    for a in soup.find_all('a', href=True):
        href = a.get('href')
        text = a.get_text(strip=True)
        if href and not href.startswith('#'):
            # 解析相对URL
            if not href.startswith(('http://', 'https://', '//')):
                from urllib.parse import urljoin
                href = urljoin(url, href)
            all_links.append(f"- [{text or '无文本'}]({href})")
    
    if all_links:
        content_parts.append("\n## 页面链接汇总\n")
        content_parts.extend(all_links[:100])  # 限制最多100个链接
        if len(all_links) > 100:
            content_parts.append(f"\n... 还有 {len(all_links) - 100} 个链接未显示")
    
    # 3.4 提取所有图片
    all_images = []
    for img in soup.find_all('img', src=True):
        src = img.get('src')
        alt = img.get('alt', '')
        if src and not src.startswith('data:'):
            # 解析相对URL
            if not src.startswith(('http://', 'https://', '//')):
                from urllib.parse import urljoin
                src = urljoin(url, src)
            all_images.append(src)
    
    if all_images:
        content_parts.append("\n## 页面图片汇总\n")
        for img_url in all_images[:50]:  # 限制最多50张图片
            content_parts.append(f"![]({img_url})")
        if len(all_images) > 50:
            content_parts.append(f"\n... 还有 {len(all_images) - 50} 张图片未显示")
    
    # 4. 构建最终的Markdown
    current_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    date_only = datetime.datetime.now().strftime('%Y-%m-%d')
    
    md_lines = [
        f"# {title}",
        "",
        "## 文档信息",
        f"- 标题: {title}",
        f"- 来源: [{url}]({url})",
        f"- 抓取时间: {current_time}",
        f"- 解析模式: Raw (完整内容模式)",
        "",
        *content_parts
    ]
    
    # 5. 构建元数据
    metadata = {
        'parser': 'raw',
        'parser_used': parser_used,
        'title': title,
        'url': url,
        'scraped_at': current_time,
        'content_length': len(html),
        'text_length': sum(len(part) for part in content_parts if isinstance(part, str)),
        'images_count': len(all_images),
        'links_count': len(all_links),
        'description': f'Raw mode extraction using {parser_used} - complete content preservation'
    }
    
    return date_only, '\n'.join(md_lines), metadata


def generic_to_markdown(html: str, url: str) -> tuple[str, str, dict]:
    title = extract_meta(html, 'og:title') or extract_meta(html, 'twitter:title')
    if not title:
        m = re.search(r'<title[^>]*>(.*?)</title>', html, re.I|re.S)
        if m:
            title = ihtml.unescape(re.sub(r'<[^>]+>', '', m.group(1))).strip()
    title = title or '未命名'
    date_only, date_time = parse_date_like(extract_meta(html, 'article:published_time') or extract_meta(html, 'og:updated_time'))
    
    # Priority 1: Try JSON-LD extraction first
    desc = ''
    json_ld = extract_json_ld_content(html)
    if json_ld.get('articleBody'):
        desc = json_ld['articleBody']
    elif json_ld.get('description'):
        desc = json_ld['description']
    
    # Priority 2: Try to extract from multiple <p> tags (for sites like ebchina.com)
    if not desc:
        # Extract all paragraph content with various styles
        p_patterns = [
            r'<p[^>]*class=["\']p["\'][^>]*>(.*?)</p>',  # class="p"
            r'<p[^>]*style=["\'][^"\']*text-align[^"\']*["\'][^>]*>(.*?)</p>',  # style with text-align
        ]
        
        all_paragraphs = []
        for pattern in p_patterns:
            p_matches = re.findall(pattern, html, re.I|re.S)
            for p in p_matches:
                # Clean HTML but preserve structure indicators
                text = re.sub(r'<video[^>]*>.*?</video>', '[视频]', p, flags=re.I|re.S)
                text = re.sub(r'<img[^>]*>', '', text)  # Remove img tags (will be handled separately)
                text = re.sub(r'<[^>]+>', '', text)  # Remove other HTML tags
                text = ihtml.unescape(text).strip()
                if text and len(text) > 5 and text not in all_paragraphs:  # Skip duplicates and very short text
                    all_paragraphs.append(text)
        
        if all_paragraphs:
            desc = '\n\n'.join(all_paragraphs)
    
    # Priority 3: Beijing gov site specific content divs (existing code)
    if not desc:
        for pattern in [r'<div[^>]+class=["\']view[^>]*>(.*?)</div>',
                       r'<div[^>]+class=["\']TRS_UEDITOR[^>]*>(.*?)</div>']:
            m = re.search(pattern, html, re.I|re.S)
            if m:
                desc = re.sub(r'<[^>]+>', '', m.group(1))
                desc = ihtml.unescape(desc).strip()
                break
    
    # Priority 4: Try to extract content from generic <p> tags
    if not desc:
        # Extract all generic paragraphs with substantial content
        generic_p_pattern = r'<p[^>]*>(.*?)</p>'
        generic_p_matches = re.findall(generic_p_pattern, html, re.I|re.S)
        if generic_p_matches:
            paragraphs = []
            for p in generic_p_matches:
                # Clean HTML tags
                text = re.sub(r'<img[^>]*>', '[图片]', p)  # Replace images with placeholder
                text = re.sub(r'<video[^>]*>.*?</video>', '[视频]', text, flags=re.I|re.S)  # Replace videos
                text = re.sub(r'<[^>]+>', '', text)
                text = ihtml.unescape(text).strip()
                # Only include substantial paragraphs (more than 20 chars)
                if text and len(text) > 20 and not text.startswith('var ') and not text.startswith('function'):
                    paragraphs.append(text)
            if len(paragraphs) >= 3:  # Only use if we found multiple paragraphs
                desc = '\n\n'.join(paragraphs)
    
    # Priority 5: Fallback to meta description (existing code)
    if not desc:
        desc = extract_meta(html, 'description').strip()
    
    # Update date if JSON-LD has it and no meta date was found
    original_meta_date = extract_meta(html, 'article:published_time') or extract_meta(html, 'og:updated_time')
    if json_ld.get('datePublished') and not original_meta_date:
        date_only, date_time = parse_date_like(json_ld['datePublished'])
    
    # Extract images and videos from content
    images = []
    videos = []
    
    # Find all img tags
    img_pattern = r'<img[^>]*src=["\']([^"\']+)["\'][^>]*>'
    img_matches = re.findall(img_pattern, html, re.I)
    for img_url in img_matches:
        if img_url and not img_url.startswith('data:'):
            full_url = resolve_url_with_context(url, img_url)
            if full_url not in images:
                images.append(full_url)
    
    # Find all video tags
    video_pattern = r'<video[^>]*src=["\']([^"\']+)["\'][^>]*>'
    video_matches = re.findall(video_pattern, html, re.I)
    for video_url in video_matches:
        if video_url:
            full_url = resolve_url_with_context(url, video_url)
            if full_url not in videos:
                videos.append(full_url)
    
    lines = [f"# {title}", f"- 标题: {title}", f"- 发布时间: {date_time}", f"- 来源: [{url}]({url})", f"- 抓取时间: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}", "", desc or '(未能提取正文)']
    
    # Add images section if any images found
    if images:
        lines.append("\n## 图片\n")
        for img_url in images:
            lines.append(f"![]({img_url})")
    
    # Add videos section if any videos found
    if videos:
        lines.append("\n## 视频\n")
        for video_url in videos:
            lines.append(f"[视频链接]({video_url})")
    
    metadata = {
        'description': desc,
        'images': images,
        'videos': videos,
        'publish_time': json_ld.get('datePublished') or extract_meta(html, 'article:published_time') or extract_meta(html, 'og:updated_time'),
        'author': json_ld.get('author', '')
    }
    return date_only, "\n\n".join(lines).strip() + "\n", metadata


def find_next_url(html: str, current_url: str, parser_name: str) -> Optional[str]:
    """Find next page URL based on parser type."""
    if 'mkdocs' in parser_name.lower():
        return find_mkdocs_next_url(html, current_url)
    elif 'docusaurus' in parser_name.lower():
        return find_docusaurus_next_url(html, current_url)
    return None

def find_mkdocs_next_url(html: str, current_url: str) -> Optional[str]:
    """Find next URL in MkDocs navigation."""
    patterns = [
        r'<a[^>]+class=["\'][^"\']*md-footer-nav__link--next[^"\']*["\'][^>]*href=["\']([^"\']+)["\']',
        r'<a[^>]+href=["\']([^"\']+)["\'][^>]*class=["\'][^"\']*md-footer-nav__link--next[^"\']*["\']',
        r'<a[^>]+href=["\']([^"\']+)["\'][^>]*>[\s\S]*?Next[\s\S]*?</a>'
    ]
    for pat in patterns:
        m = re.search(pat, html, re.I)
        if m:
            next_url = m.group(1)
            # Skip same-page anchors and relative anchors
            if next_url.startswith('#'):
                continue
            full_url = resolve_url_with_context(current_url, next_url)
            # Skip if it's the same URL (just with different anchor)
            if full_url.split('#')[0] == current_url.split('#')[0]:
                continue
            return full_url
    return None

def find_docusaurus_next_url(html: str, current_url: str) -> Optional[str]:
    """Find next URL in Docusaurus navigation."""
    patterns = [
        r'<a[^>]+class=["\'][^"\']*pagination-nav__link--next[^"\']*["\'][^>]*href=["\']([^"\']+)["\']',
        r'<a[^>]+href=["\']([^"\']+)["\'][^>]*class=["\'][^"\']*pagination-nav__link--next[^"\']*["\']'
    ]
    for pat in patterns:
        m = re.search(pat, html, re.I)
        if m:
            next_url = m.group(1)
            # Skip same-page anchors and relative anchors
            if next_url.startswith('#'):
                continue
            full_url = resolve_url_with_context(current_url, next_url)
            # Skip if it's the same URL (just with different anchor)
            if full_url.split('#')[0] == current_url.split('#')[0]:
                continue
            return full_url
    return None

def is_same_section(current_url: str, next_url: str) -> bool:
    """Check if next URL belongs to same documentation section."""
    c_parts = urllib.parse.urlparse(current_url)
    n_parts = urllib.parse.urlparse(next_url)
    return (c_parts.netloc == n_parts.netloc and 
            n_parts.path.startswith(c_parts.path.rsplit('/', 2)[0]))

def process_pagination(initial_url: str, initial_html: str, parser_func, ua: str) -> list:
    """Follow pagination links and collect all pages."""
    visited = set()
    pages = []
    current_url = initial_url
    current_html = initial_html
    depth = 0
    
    while depth < MAX_PAGINATION_DEPTH and current_url not in visited:
        try:
            visited.add(current_url)
            logging.info(f"Processing page {depth + 1}: {current_url}")
            pages.append(parser_func(current_html, current_url))
            
            next_url = find_next_url(current_html, current_url, parser_func.__name__)
            if not next_url or not is_same_section(current_url, next_url):
                logging.info(f"Pagination stopped: {'no next URL' if not next_url else 'different section'}")
                break
                
            logging.info(f"Following pagination to: {next_url}")
            current_html = fetch_html(next_url, ua=ua, timeout=30)
            current_url = next_url
            depth += 1
            
        except Exception as e:
            logging.warning(f"Pagination stopped at depth {depth}: {e}")
            break
    
    return pages

def aggregate_multi_page_content(pages: list) -> tuple[str, str, dict]:
    """Merge multiple page contents into single markdown document."""
    if not pages:
        return '', '', {}
    
    first_date, first_content, first_metadata = pages[0]
    if len(pages) == 1:
        return first_date, first_content, first_metadata
    
    # Combine all content
    all_content = [first_content]
    all_images = list(first_metadata.get('images', []))
    
    for i in range(1, len(pages)):
        date, content, metadata = pages[i]
        # Extract body content (skip header metadata lines)
        lines = content.split('\n')
        body_start = 0
        for j, line in enumerate(lines):
            if line.strip() and not line.startswith('- ') and not line.startswith('#'):
                body_start = j
                break
        
        body_content = '\n'.join(lines[body_start:]).strip()
        if body_content:
            all_content.append(f"\n---\n\n{body_content}")
        
        # Aggregate images
        all_images.extend(metadata.get('images', []))
    
    # Create combined metadata
    combined_metadata = first_metadata.copy()
    combined_metadata['images'] = list(set(all_images))
    combined_metadata['pages_count'] = len(pages)
    
    return first_date, '\n'.join(all_content), combined_metadata


def normalize_url_for_dedup(url: str) -> str:
    """Normalize URL for deduplication: lowercase scheme and netloc only, preserve path case, remove fragments, sort query params."""
    parsed = urllib.parse.urlparse(url)
    
    # Only lowercase the scheme and netloc (domain), preserve path case
    scheme = parsed.scheme.lower()
    netloc = parsed.netloc.lower()
    
    # Sort query parameters alphabetically
    query_params = urllib.parse.parse_qsl(parsed.query, keep_blank_values=True)
    sorted_query = urllib.parse.urlencode(sorted(query_params))
    
    # Normalize path (remove trailing slash except for root) - preserve case
    path = parsed.path.rstrip('/') if parsed.path != '/' else parsed.path
    
    # Reconstruct URL without fragment - preserving original case for path, params
    normalized = urllib.parse.urlunparse((
        scheme, netloc, path, 
        parsed.params, sorted_query, ''
    ))
    return normalized

def should_crawl_url(url: str) -> bool:
    """Smart filtering to skip binary files, APIs, and build artifacts."""
    url_lower = url.lower()
    parsed = urllib.parse.urlparse(url_lower)
    path = parsed.path
    
    # Check file extensions
    for ext_set in [BINARY_EXTENSIONS, IMAGE_EXTENSIONS, VIDEO_EXTENSIONS, AUDIO_EXTENSIONS]:
        if any(path.endswith(ext) for ext in ext_set):
            return False
    
    # Check API and build patterns
    for pattern_set in [API_PATTERNS, BUILD_PATTERNS]:
        if any(pattern in url_lower for pattern in pattern_set):
            return False
    
    return True

def should_preserve_subdirectory(base_url: str) -> bool:
    """Determine if the base URL represents a subdirectory deployment that should be preserved."""
    base_parts = urllib.parse.urlparse(base_url)
    path_segments = [s for s in base_parts.path.strip('/').split('/') if s]
    
    if not path_segments:
        return False
    
    first_segment = path_segments[0]
    
    # GitHub Pages subdirectory deployments
    if base_parts.netloc.endswith('.github.io'):
        return True
    
    # Documentation sites with 'docs' in path
    if 'docs' in first_segment.lower():
        return True
    
    # Multi-level paths indicate subdirectory deployment
    if len(path_segments) > 1:
        return True
    
    # Sites with common project/documentation patterns
    doc_patterns = ['project', 'manual', 'guide', 'api', 'reference', 'book']
    if any(pattern in first_segment.lower() for pattern in doc_patterns):
        return True
    
    return False

def resolve_url_with_context(base_url: str, href: str) -> str:
    """Smart URL resolution that preserves subdirectory context with enhanced edge case handling."""
    base_parts = urllib.parse.urlparse(base_url)
    
    # Handle protocol-relative URLs (starting with //)
    if href.startswith('//'):
        return f"{base_parts.scheme}:{href}"
    
    # Special case: root navigation '/' should go to domain root
    if href == '/':
        return f"{base_parts.scheme}://{base_parts.netloc}/"
    
    # Detect subdirectory deployment
    path_segments = [s for s in base_parts.path.strip('/').split('/') if s]
    
    # If href starts with '/' and we have a subdirectory, preserve the subdirectory context
    if href.startswith('/') and path_segments and should_preserve_subdirectory(base_url):
        first_segment = path_segments[0]
        
        # Skip if href already includes the subdirectory
        if not href.startswith('/' + first_segment + '/') and href != '/' + first_segment:
            # Preserve subdirectory context
            return f"{base_parts.scheme}://{base_parts.netloc}/{first_segment}{href}"
    
    # CRITICAL FIX: For relative paths (not starting with /), ensure base URL has trailing slash
    # This is essential for correct urljoin behavior with subdirectory deployments
    if not href.startswith('/') and base_parts.path and not base_parts.path.endswith('/'):
        # Check if this looks like a directory URL (common for documentation sites)
        # A URL without extension or ending with known directory patterns should be treated as directory
        if not any(base_parts.path.endswith(ext) for ext in ['.html', '.htm', '.php', '.asp', '.jsp']):
            # Add trailing slash to base URL for correct relative resolution
            base_url_fixed = base_url + '/'
            return urllib.parse.urljoin(base_url_fixed, href)
    
    # Default to standard urljoin for other cases
    return urllib.parse.urljoin(base_url, href)

def extract_internal_links(html: str, base_url: str) -> dict:
    """Extract all internal links from HTML content with smart subdirectory resolution.
    Returns dict mapping normalized URLs to original URLs for case-preserving fetching."""
    links = {}  # normalized_url -> original_url
    base_parts = urllib.parse.urlparse(base_url)
    
    # Find all href attributes
    href_pattern = r'href=["\']([^"\']*)["\']'
    for match in re.finditer(href_pattern, html, re.I):
        href = match.group(1)
        
        # Skip empty hrefs or anchors, javascript, mailto
        if not href or href.startswith(('#', 'javascript:', 'mailto:', 'tel:')):
            continue
            
        # Convert to absolute URL using smart context resolution
        full_url = resolve_url_with_context(base_url, href)
        url_parts = urllib.parse.urlparse(full_url)
        
        # Check if same domain and should crawl
        if url_parts.netloc == base_parts.netloc and should_crawl_url(full_url):
            # Map normalized URL to original URL for case-preserving fetching
            normalized = normalize_url_for_dedup(full_url)
            links[normalized] = full_url
    
    return links

def is_documentation_url(url: str) -> bool:
    """Filter URLs to likely documentation pages."""
    # Skip common non-doc patterns
    skip_patterns = [
        r'/api/', r'/download', r'\.zip$', r'\.tar', r'\.pdf$',
        r'/signin', r'/login', r'/auth', r'/search\?',
        r'\.xml$', r'\.json$', r'/feed', r'/rss'
    ]
    
    for pattern in skip_patterns:
        if re.search(pattern, url, re.I):
            return False
    
    return True  # Default: include

def crawl_site(start_url: str, ua: str, max_depth: int = 10, 
               max_pages: int = 1000, delay: float = 0.5) -> list:
    """
    Crawl entire site using BFS algorithm.
    Returns list of (url, html, depth) tuples.
    """
    # Initialize crawl statistics
    stats = {
        'pages_crawled': 0,
        'pages_success': 0, 
        'pages_failed': 0,
        'total_size': 0,
        'start_time': time.time(),
        'failed_urls': []  # Track failed URLs for detailed reporting
    }
    
    visited_normalized = set()  # For deduplication using normalized URLs
    url_mapping = {}  # Maps normalized URLs to original URLs for fetching
    queue = deque([(start_url, 0)])  # (original_url, depth) - keep original URL
    pages = []
    
    logging.info(f"Starting site crawl from {start_url}")
    logging.info(f"Settings: max_depth={max_depth}, max_pages={max_pages}, delay={delay}s")
    
    while queue and len(visited_normalized) < max_pages:
        current_url, depth = queue.popleft()
        current_normalized = normalize_url_for_dedup(current_url)
        
        # Skip if already visited or too deep
        if current_normalized in visited_normalized or depth > max_depth:
            continue
        
        # Rate limiting
        if visited_normalized and delay > 0:
            time.sleep(delay)
        
        stats['pages_crawled'] += 1
        
        try:
            # Progress reporting: verbose logging vs progress line
            if logging.getLogger().level <= logging.INFO:
                # Verbose mode: full logging
                logging.info(f"[{len(visited_normalized)+1}/{max_pages}] Crawling depth {depth}: {current_url}")
            else:
                # Normal mode: updating progress line on stderr
                elapsed = time.time() - stats['start_time']
                rate = stats['pages_success'] / (elapsed / 60) if elapsed > 0 else 0  # pages per minute
                
                # Progress line that overwrites itself
                sys.stderr.write(f"\rCrawling: {len(visited_normalized)+1}/{max_pages} pages ({rate:.1f} pages/min)")
                sys.stderr.flush()
            
            # Fetch page using original URL (preserves case)
            html = fetch_html(current_url, ua=ua, timeout=30)
            visited_normalized.add(current_normalized)
            url_mapping[current_normalized] = current_url
            pages.append((current_url, html, depth))
            
            # Update statistics
            stats['pages_success'] += 1
            stats['total_size'] += len(html.encode('utf-8'))
            
            # Extract and queue new links (only if not at max depth)
            if depth < max_depth:
                link_mapping = extract_internal_links(html, current_url)
                new_normalized_links = set(link_mapping.keys()) - visited_normalized
                
                # Filter to documentation URLs (check both normalized and original)
                doc_links = [(norm, orig) for norm, orig in link_mapping.items() 
                           if norm in new_normalized_links and is_documentation_url(orig)]
                
                logging.info(f"Found {len(doc_links)} new documentation links")
                
                for normalized_link, original_link in sorted(doc_links)[:50]:  # Limit per-page discoveries
                    # Queue the original URL for case-preserving fetching
                    queue.append((original_link, depth + 1))
            
        except Exception as e:
            logging.warning(f"Failed to crawl {current_url}: {e}")
            stats['pages_failed'] += 1
            stats['failed_urls'].append((current_url, str(e)))
            continue
    
    # Clear progress line if we were showing it
    if logging.getLogger().level > logging.INFO:
        sys.stderr.write('\r' + ' ' * 80 + '\r')  # Clear the line
        sys.stderr.flush()
    
    # Enhanced crawl summary and visibility reporting
    duration = time.time() - stats['start_time']
    size_mb = stats['total_size'] / (1024 * 1024)
    success_rate = (stats['pages_success'] / stats['pages_crawled'] * 100) if stats['pages_crawled'] > 0 else 0
    
    # 1. Crawl quality summary (5-8 lines)
    logging.info(f"Crawl Quality Summary: {success_rate:.1f}% success rate ({stats['pages_success']}/{stats['pages_crawled']} pages)")
    logging.info(f"Data Retrieved: {size_mb:.1f}MB in {duration:.1f}s ({size_mb/duration:.2f} MB/s)")
    
    # 2. Failed URL details in verbose mode (3-5 lines)
    if stats['failed_urls'] and logging.getLogger().level <= logging.INFO:
        logging.info(f"Failed URLs ({len(stats['failed_urls'])}):") 
        for failed_url, error in stats['failed_urls'][:5]:  # Show first 5 failures
            logging.info(f"  - {failed_url}: {error}")
        if len(stats['failed_urls']) > 5:
            logging.info(f"  ... and {len(stats['failed_urls']) - 5} more failures")
    
    # 3. Completeness indicator (2-3 lines)
    hit_max_pages = len(visited_normalized) >= max_pages
    hit_max_depth = any(depth >= max_depth for _, _, depth in pages)
    if hit_max_pages or hit_max_depth:
        limits_hit = []
        if hit_max_pages: limits_hit.append(f"max-pages({max_pages})")
        if hit_max_depth: limits_hit.append(f"max-depth({max_depth})")
        logging.info(f"Crawl Status: INCOMPLETE - stopped due to {' and '.join(limits_hit)} limit")
    else:
        logging.info("Crawl Status: COMPLETE - all discoverable pages crawled")
    
    return pages

def aggregate_crawled_site(pages: list, parser_func) -> tuple[str, str, dict]:
    """
    Aggregate crawled site pages into single comprehensive document.
    Organizes content by depth and URL structure.
    """
    if not pages:
        return '', '', {}
    
    # Group pages by depth for hierarchical organization
    by_depth = {}
    for url, html, depth in pages:
        if depth not in by_depth:
            by_depth[depth] = []
        by_depth[depth].append((url, html))
    
    # Parse all pages
    all_content = []
    all_images = []
    toc_entries = []
    
    for depth in sorted(by_depth.keys()):
        if depth > 0:
            all_content.append(f"\n{'#' * (depth + 1)} Level {depth} Pages\n")
        
        for url, html in by_depth[depth]:
            try:
                date, content, metadata = parser_func(html, url)
                
                # Extract title from content
                title_match = re.search(r'^#\s+(.+)$', content, re.M)
                title = title_match.group(1) if title_match else urllib.parse.urlparse(url).path
                
                # Add to TOC
                indent = '  ' * depth
                toc_entries.append(f"{indent}- [{title}](#{depth}-{len(toc_entries)})")
                
                # Add content with section anchor
                all_content.append(f"\n<a id='{depth}-{len(toc_entries)-1}'></a>\n")
                all_content.append(content)
                all_content.append("\n---\n")
                
                # Collect images
                all_images.extend(metadata.get('images', []))
                
            except Exception as e:
                logging.warning(f"Failed to parse {url}: {e}")
    
    # Build final document with TOC
    toc = "## Table of Contents\n\n" + '\n'.join(toc_entries)
    final_content = toc + "\n\n" + '\n'.join(all_content)
    
    # Create metadata
    metadata = {
        'total_pages': len(pages),
        'max_depth': max(by_depth.keys()) if by_depth else 0,
        'images': list(set(all_images)),
        'crawl_complete': True
    }
    
    return datetime.datetime.now().strftime("%Y-%m-%d"), final_content, metadata


def rewrite_and_download_assets(md: str, md_base: str, outdir: Path, ua: str, assets_root: str) -> str:
    # Find all http(s) images
    urls = []
    for m in re.finditer(r'!\[[^\]]*\]\((https?://[^)]+)\)', md, re.I):
        url = m.group(1)
        if url not in urls:
            urls.append(url)
    # Also capture regular links that are image-like
    for m in re.finditer(r'\[[^\]]*\]\((https?://[^)]+)\)', md, re.I):
        url = m.group(1)
        if url in urls:
            continue
        if re.search(r'\.(?:jpg|jpeg|png|webp|gif)(?:\?|$)', url, re.I) or ('imageMogr2' in url) or ('imageView2' in url):
            urls.append(url)
    if not urls:
        return md
    # Prepare asset directory
    assets_dir = outdir / assets_root / sanitize_filename(md_base)
    assets_dir.mkdir(parents=True, exist_ok=True)

    def filename_for(i: int, url: str) -> str:
        # derive extension from URL path; fallback to .jpg
        path = urllib.parse.urlparse(url).path
        ext = ''
        m = re.search(r'\.([a-zA-Z0-9]{3,4})$', path)
        if m:
            ext = '.' + m.group(1).lower()
        elif 'imageMogr2' in url or 'imageView2' in url:
            ext = '.jpg'
        else:
            ext = '.jpg'
        return f"{i:02d}{ext}"

    # Download and build mapping
    mapping = {}
    for idx, u in enumerate(urls, start=1):
        fname = filename_for(idx, u)
        dest = assets_dir / fname
        if not dest.exists():
            # download with UA
            try:
                req = urllib.request.Request(u, headers={"User-Agent": ua, "Accept-Language": "zh-CN,zh;q=0.9"})
                with urllib.request.urlopen(req, timeout=60, context=ssl_context_unverified) as r:
                    data = r.read()
                dest.write_bytes(data)
            except Exception:
                # skip failures; leave URL as is
                continue
        rel_path = os.path.relpath(dest, outdir)
        mapping[u] = rel_path

    # Replace in Markdown
    def repl_img(m):
        u = m.group(1)
        return m.group(0).replace(u, mapping.get(u, u))
    def repl_link(m):
        u = m.group(2)
        if u in mapping:
            return f"[{m.group(1)}]({mapping[u]})"
        return m.group(0)
    md2 = re.sub(r'!\[[^\]]*\]\((https?://[^)]+)\)', repl_img, md)
    md2 = re.sub(r'\[([^\]]*)\]\((https?://[^)]+)\)', repl_link, md2)
    return md2


def main():
    ap = argparse.ArgumentParser(
        description='Fetch a URL (WeChat/XHS/generic) and save as Markdown.',
        prog='webfetcher'
    )
    ap.add_argument('url', help='Target URL')
    ap.add_argument('--version', action='version', version=f'%(prog)s {__version__}')
    ap.add_argument('-o','--outdir', default='.', help='Output directory (default: current)')
    ap.add_argument('--render', choices=['auto','always','never'], default='auto', help='Use headless rendering (default: auto)')
    ap.add_argument('--timeout', type=int, default=60, help='Network timeout in seconds (fetch). Default: 60')
    ap.add_argument('--render-timeout', type=int, default=90, help='Rendering timeout in seconds (Playwright). Default: 90')
    ap.add_argument('--html', help='Use local HTML file instead of fetching/rendering')
    ap.add_argument('--download-assets', action='store_true', help='Download images to assets/<file>/ and rewrite links')
    ap.add_argument('--assets-root', default='assets', help='Assets root directory name (default: assets)')
    ap.add_argument('--save-html', nargs='?', const=True, help='Save fetched/rendered HTML snapshot before parsing (optional path).')
    ap.add_argument('--json', action='store_true', help='Output structured JSON alongside Markdown')
    ap.add_argument('--verbose', action='store_true', help='Enable verbose logging (INFO level)')
    ap.add_argument('--follow-pagination', action='store_true', 
                    help='Follow next/previous links to aggregate multi-page documents (MkDocs/Docusaurus only)')
    ap.add_argument('--raw', action='store_true', 
                    help='Use raw parser mode (complete content preservation, no filtering)')
    ap.add_argument('--crawl-site', action='store_true',
                    help='Recursively crawl entire site (BFS traversal of all internal links)')
    ap.add_argument('--max-crawl-depth', type=int, default=10,
                    help='Maximum crawl depth for site crawling (default: 10, max: 10)')
    ap.add_argument('--max-pages', type=int, default=1000,
                    help='Maximum pages to crawl (default: 1000, max: 1000)')
    ap.add_argument('--crawl-delay', type=float, default=0.5,
                    help='Delay between crawl requests in seconds (default: 0.5)')
    args = ap.parse_args()
    
    # Validate crawl limits against absolute maximums
    if args.max_crawl_depth > MAX_CRAWL_DEPTH:
        logging.warning(f"Requested depth {args.max_crawl_depth} exceeds maximum {MAX_CRAWL_DEPTH}, using {MAX_CRAWL_DEPTH}")
        args.max_crawl_depth = MAX_CRAWL_DEPTH
    if args.max_pages > MAX_CRAWL_PAGES:
        logging.warning(f"Requested pages {args.max_pages} exceeds maximum {MAX_CRAWL_PAGES}, using {MAX_CRAWL_PAGES}")
        args.max_pages = MAX_CRAWL_PAGES
    
    setup_logging(args.verbose)
    url = args.url
    outdir = Path(args.outdir)
    outdir.mkdir(parents=True, exist_ok=True)
    
    logging.info(f"Starting webfetcher for URL: {url}")

    host = urllib.parse.urlparse(url).hostname or ''
    ua = "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0 Safari/537.36"
    # Use a mobile WeChat UA for WeChat pages; mobile UA for XHS
    if 'mp.weixin.qq.com' in host or 'weixin.qq.com' in host:
        ua = 'Mozilla/5.0 (iPhone; CPU iPhone OS 16_5 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148 MicroMessenger/8.0.42(0x18002a2c) NetType/WIFI Language/zh_CN'
    elif 'xiaohongshu.com' in host:
        ua = 'Mozilla/5.0 (iPhone; CPU iPhone OS 16_5 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148 MicroMessenger/8.0.42(0x18002a2c) NetType/WIFI Language/zh_CN'
    elif 'dianping.com' in host:
        ua = 'Mozilla/5.0 (iPhone; CPU iPhone OS 16_5 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.0 Mobile/15E148 Safari/604.1'

    # Site crawling mode (overrides single-page fetch)
    if args.crawl_site:
        logging.info("Site crawling mode activated")
        
        # Check if supported site type
        if 'mp.weixin.qq.com' in host or 'xiaohongshu.com' in host or 'dianping.com' in host:
            logging.error("Site crawling not supported for social media sites")
            sys.exit(1)
        
        # Crawl the site
        crawled_pages = crawl_site(
            url, ua, 
            max_depth=args.max_crawl_depth,
            max_pages=args.max_pages,
            delay=args.crawl_delay
        )
        
        if crawled_pages:
            # Detect appropriate parser from first page
            first_html = crawled_pages[0][1]
            if args.raw:
                parser_func = raw_to_markdown
                parser_name = "Raw"
                logging.info("Using Raw parser for site content (user requested)")
            elif re.search(r'theme-doc-markdown|class="[^"]*\\bmarkdown\\b', first_html, re.I):
                parser_func = docusaurus_to_markdown
                parser_name = "Docusaurus"
            elif re.search(r'md-content__inner\s+md-typeset', first_html, re.I):
                parser_func = mkdocs_to_markdown
                parser_name = "MkDocs"
            else:
                parser_func = generic_to_markdown
                parser_name = "Generic"
            
            logging.info(f"Using {parser_name} parser for site content")
            
            # Aggregate all content
            date_only, md, metadata = aggregate_crawled_site(crawled_pages, parser_func)
            metadata['parser_used'] = parser_name
            rendered = False
            
            # Process and save file directly in crawl mode
            # Title for filename comes from first heading
            m = re.match(r'^#\s*(.+)$', md.splitlines()[0].strip())
            title = m.group(1) if m else '未命名'
            # Use current timestamp for filename to avoid conflicts
            timestamp = datetime.datetime.now().strftime("%Y-%m-%d-%H%M%S")
            base = f"{timestamp} - {sanitize_filename(title)}"
            path = ensure_unique_path(outdir, base)
            
            # Optionally download images and rewrite links
            do_download_assets = args.download_assets or ('mp.weixin.qq.com' in host) or ('xiaohongshu.com' in host)
            if do_download_assets:
                logging.info("Starting asset downloads")
                md_base = base  # same base as filename
                md = rewrite_and_download_assets(md, md_base, outdir, ua, args.assets_root)
                logging.info("Asset downloads completed")
            
            path.write_text(md, encoding='utf-8')
            logging.info(f"Markdown file saved: {path}")
            
            # Generate JSON output if requested
            if args.json:
                json_data = {
                    'url': url,
                    'title': title,
                    'date': f"{date_only} {datetime.datetime.now().strftime('%H:%M:%S')}",
                    'content': md,
                    'images': metadata.get('images', []),
                    'metadata': {
                        **metadata,
                        'parser_used': parser_name,
                        'fetch_method': 'crawl',
                        'scraped_at': datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                    }
                }
                json_path = path.with_suffix('.json')
                json_path.write_text(json.dumps(json_data, ensure_ascii=False, indent=2), encoding='utf-8')
                logging.info(f"JSON data saved: {json_path}")
            
            print(str(path))
            return  # Exit the main function after crawling is complete
            
        else:
            logging.error("No pages crawled successfully")
            sys.exit(1)
    elif args.html:
        html = Path(args.html).read_text(encoding='utf-8', errors='ignore')
    else:
        html = None
        should_render = (args.render == 'always') or (args.render == 'auto' and ('xiaohongshu.com' in host or 'dianping.com' in host))
        logging.info(f"Render decision: {'will render' if should_render else 'static fetch only'}")
        if should_render:
            logging.info("Attempting headless rendering with Playwright")
            rendered = try_render(url, ua=ua, timeout_ms=max(1000, args.render_timeout*1000))
            if rendered:
                logging.info("Rendering successful")
                html = rendered
            else:
                logging.info("Rendering failed, falling back to static fetch")
        if html is None:
            logging.info("Fetching HTML statically")
            html = fetch_html(url, ua=ua, timeout=args.timeout)
            logging.info("Static fetch completed")

    # Optionally save HTML snapshot before parsing
    if args.save_html:
        if args.save_html is True:
            ts = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
            host_safe = (urllib.parse.urlparse(url).hostname or 'page').replace(':','_')
            snapshot_path = Path(args.outdir) / f"snapshot_{host_safe}_{ts}.html"
        else:
            snapshot_path = Path(str(args.save_html))
            if snapshot_path.is_dir():
                ts = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
                host_safe = (urllib.parse.urlparse(url).hostname or 'page').replace(':','_')
                snapshot_path = snapshot_path / f"snapshot_{host_safe}_{ts}.html"
        try:
            snapshot_path.parent.mkdir(parents=True, exist_ok=True)
            snapshot_path.write_text(html, encoding='utf-8')
            logging.info(f"HTML snapshot saved to: {snapshot_path}")
        except Exception as e:
            logging.warning(f"Failed to save HTML snapshot: {e}")

    # Parser selection
    if args.raw:
        logging.info("Selected parser: Raw (user requested)")
        parser_name = "Raw"
        date_only, md, metadata = raw_to_markdown(html, url)
        rendered = False
    elif 'mp.weixin.qq.com' in host:
        logging.info("Selected parser: WeChat")
        parser_name = "WeChat"
        date_only, md, metadata = wechat_to_markdown(html, url)
        rendered = 'wechat' in ua.lower()
    elif 'xiaohongshu.com' in host:
        logging.info("Selected parser: Xiaohongshu")
        parser_name = "Xiaohongshu"
        date_only, md, metadata = xhs_to_markdown(html, url)
        rendered = should_render
    elif 'dianping.com' in host:
        logging.info("Selected parser: Dianping")
        parser_name = "Dianping"
        date_only, md, metadata = dianping_to_markdown(html, url)
        rendered = should_render
    elif 'ebchina.com' in host and 'class="N_title"' in html:
        # EB China news list page
        logging.info("Selected parser: EB China News List")
        parser_name = "EBChina_NewsList"
        date_only, md, metadata = ebchina_news_list_to_markdown(html, url)
        rendered = False
    elif re.search(r'theme-doc-markdown|class=\"[^\"]*\\bmarkdown\\b', html, re.I):
        logging.info("Selected parser: Docusaurus")
        parser_name = "Docusaurus"
        # Multi-page support for Docusaurus
        if args.follow_pagination:
            logging.info("Multi-page mode enabled, following pagination links...")
            pages = process_pagination(url, html, docusaurus_to_markdown, ua)
            if len(pages) > 1:
                logging.info(f"Aggregated {len(pages)} pages into single document")
                date_only, md, metadata = aggregate_multi_page_content(pages)
            else:
                date_only, md, metadata = pages[0]
        else:
            date_only, md, metadata = docusaurus_to_markdown(html, url)
        rendered = False
    elif re.search(r'md-content__inner\s+md-typeset', html, re.I):
        logging.info("Selected parser: MkDocs")
        parser_name = "MkDocs"
        # Multi-page support for MkDocs
        if args.follow_pagination:
            logging.info("Multi-page mode enabled, following pagination links...")
            pages = process_pagination(url, html, mkdocs_to_markdown, ua)
            if len(pages) > 1:
                logging.info(f"Aggregated {len(pages)} pages into single document")
                date_only, md, metadata = aggregate_multi_page_content(pages)
            else:
                date_only, md, metadata = pages[0]
        else:
            date_only, md, metadata = mkdocs_to_markdown(html, url)
        rendered = False
    else:
        logging.info("Selected parser: Generic")
        parser_name = "Generic"
        date_only, md, metadata = generic_to_markdown(html, url)
        rendered = False

    # Title for filename comes from first heading
    m = re.match(r'^#\s*(.+)$', md.splitlines()[0].strip())
    title = m.group(1) if m else '未命名'
    # Use current timestamp for filename to avoid conflicts
    timestamp = datetime.datetime.now().strftime("%Y-%m-%d-%H%M%S")
    base = f"{timestamp} - {sanitize_filename(title)}"
    path = ensure_unique_path(outdir, base)
    # Optionally download images and rewrite links
    do_download_assets = args.download_assets or ('mp.weixin.qq.com' in host) or ('xiaohongshu.com' in host)
    if do_download_assets:
        logging.info("Starting asset downloads")
        md_base = base  # same base as filename (includes timestamp)
        md = rewrite_and_download_assets(md, md_base, outdir, ua, args.assets_root)
        logging.info("Asset downloads completed")
    
    path.write_text(md, encoding='utf-8')
    logging.info(f"Markdown file saved: {path}")
    
    # Generate JSON output if requested
    if args.json:
        json_data = {
            'url': url,
            'title': title,
            'date': f"{date_only} {datetime.datetime.now().strftime('%H:%M:%S')}",
            'content': md,
            'images': metadata.get('images', []),
            'metadata': {
                **metadata,
                'parser_mode': parser_name,
                'fetch_method': 'rendered' if rendered else 'static',
                'scraped_at': datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            }
        }
        json_path = path.with_suffix('.json')
        json_path.write_text(json.dumps(json_data, ensure_ascii=False, indent=2), encoding='utf-8')
        logging.info(f"JSON data saved: {json_path}")
    
    print(str(path))


if __name__ == '__main__':
    main()
