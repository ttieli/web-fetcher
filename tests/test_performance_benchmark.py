#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ€§èƒ½åŸºå‡†æµ‹è¯•
Performance Benchmark Test for Selenium Integration
"""

import sys
import os
import time
import statistics
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from selenium_fetcher import SeleniumFetcher

def run_performance_test():
    """è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•"""
    print("\nâš¡ æ€§èƒ½åŸºå‡†æµ‹è¯•")
    print("="*60)

    fetcher = None
    results = {
        "timestamp": datetime.now().isoformat(),
        "connection_tests": [],
        "fetch_tests": [],
        "stability_tests": []
    }

    try:
        # åˆå§‹åŒ–
        fetcher = SeleniumFetcher()

        # æµ‹è¯•1ï¼šè¿æ¥ç¨³å®šæ€§æµ‹è¯•ï¼ˆ10æ¬¡è¿ç»­è¿æ¥ï¼‰
        print("\nğŸ“ æµ‹è¯•1: è¿æ¥ç¨³å®šæ€§æµ‹è¯•ï¼ˆ10æ¬¡ï¼‰")
        connection_times = []

        for i in range(10):
            start = time.time()
            success, message = fetcher.connect_to_chrome()
            duration = time.time() - start

            if success:
                connection_times.append(duration)
                print(f"   [{i+1:2d}] âœ… è¿æ¥æˆåŠŸ: {duration:.3f}ç§’")
            else:
                print(f"   [{i+1:2d}] âŒ è¿æ¥å¤±è´¥: {message}")

            # çŸ­æš‚å»¶è¿Ÿé¿å…è¿‡å¿«é‡è¿
            time.sleep(0.1)

        if connection_times:
            avg_time = statistics.mean(connection_times)
            min_time = min(connection_times)
            max_time = max(connection_times)
            std_dev = statistics.stdev(connection_times) if len(connection_times) > 1 else 0

            print(f"\n   ğŸ“Š è¿æ¥æ€§èƒ½ç»Ÿè®¡:")
            print(f"      - æˆåŠŸç‡: {len(connection_times)}/10")
            print(f"      - å¹³å‡æ—¶é—´: {avg_time:.3f}ç§’")
            print(f"      - æœ€å¿«: {min_time:.3f}ç§’")
            print(f"      - æœ€æ…¢: {max_time:.3f}ç§’")
            print(f"      - æ ‡å‡†å·®: {std_dev:.3f}ç§’")

            results["connection_tests"] = {
                "success_rate": f"{len(connection_times)}/10",
                "average": avg_time,
                "min": min_time,
                "max": max_time,
                "std_dev": std_dev,
                "samples": connection_times
            }

        # æµ‹è¯•2ï¼šé¡µé¢æŠ“å–æ€§èƒ½æµ‹è¯•
        print("\nğŸ“ æµ‹è¯•2: é¡µé¢æŠ“å–æ€§èƒ½æµ‹è¯•ï¼ˆ5ä¸ªé¡µé¢ï¼‰")

        # ç¡®ä¿å·²è¿æ¥
        fetcher.connect_to_chrome()

        test_urls = [
            "http://example.com",
            "http://example.org",
            "http://example.net",
            "http://httpbin.org/html",
            "http://httpbin.org/status/200"
        ]

        fetch_times = []
        for i, url in enumerate(test_urls, 1):
            start = time.time()
            content, metrics = fetcher.fetch_html_selenium(url)
            duration = time.time() - start

            if content:
                fetch_times.append({
                    "url": url,
                    "total_time": duration,
                    "page_load_time": metrics.page_load_time,
                    "content_size": len(content)
                })
                print(f"   [{i}] âœ… {url[:30]:30s} {duration:.3f}ç§’ ({len(content):,}å­—èŠ‚)")
            else:
                print(f"   [{i}] âŒ {url[:30]:30s} å¤±è´¥")

        if fetch_times:
            avg_fetch = statistics.mean(t["total_time"] for t in fetch_times)
            avg_load = statistics.mean(t["page_load_time"] for t in fetch_times)

            print(f"\n   ğŸ“Š æŠ“å–æ€§èƒ½ç»Ÿè®¡:")
            print(f"      - æˆåŠŸç‡: {len(fetch_times)}/{len(test_urls)}")
            print(f"      - å¹³å‡æ€»æ—¶é—´: {avg_fetch:.3f}ç§’")
            print(f"      - å¹³å‡åŠ è½½æ—¶é—´: {avg_load:.3f}ç§’")
            print(f"      - æ€»æ•°æ®é‡: {sum(t['content_size'] for t in fetch_times):,}å­—èŠ‚")

            results["fetch_tests"] = {
                "success_rate": f"{len(fetch_times)}/{len(test_urls)}",
                "average_total": avg_fetch,
                "average_load": avg_load,
                "details": fetch_times
            }

        # æµ‹è¯•3ï¼šè¿ç»­è´Ÿè½½æµ‹è¯•
        print("\nğŸ“ æµ‹è¯•3: è¿ç»­è´Ÿè½½æµ‹è¯•ï¼ˆ20æ¬¡å¿«é€ŸæŠ“å–ï¼‰")

        rapid_test_url = "http://example.com"
        rapid_times = []
        failures = 0

        start_batch = time.time()
        for i in range(20):
            start = time.time()
            content, metrics = fetcher.fetch_html_selenium(rapid_test_url)
            duration = time.time() - start

            if content:
                rapid_times.append(duration)
                if (i + 1) % 5 == 0:
                    print(f"   [{i+1:2d}] âœ… æ‰¹æ¬¡å®Œæˆ: å¹³å‡{statistics.mean(rapid_times[-5:]):.3f}ç§’")
            else:
                failures += 1

        total_batch_time = time.time() - start_batch

        if rapid_times:
            print(f"\n   ğŸ“Š è´Ÿè½½æµ‹è¯•ç»Ÿè®¡:")
            print(f"      - æˆåŠŸç‡: {len(rapid_times)}/20")
            print(f"      - å¤±è´¥æ¬¡æ•°: {failures}")
            print(f"      - æ€»è€—æ—¶: {total_batch_time:.2f}ç§’")
            print(f"      - å¹³å‡å“åº”: {statistics.mean(rapid_times):.3f}ç§’")
            print(f"      - ååé‡: {len(rapid_times)/total_batch_time:.2f}æ¬¡/ç§’")

            results["stability_tests"] = {
                "success_rate": f"{len(rapid_times)}/20",
                "failures": failures,
                "total_time": total_batch_time,
                "average_response": statistics.mean(rapid_times),
                "throughput": len(rapid_times)/total_batch_time
            }

    except Exception as e:
        print(f"\nâŒ æµ‹è¯•é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()

    finally:
        if fetcher:
            try:
                if hasattr(fetcher, 'close'):
                    fetcher.close()
            except:
                pass

    # ç”Ÿæˆæ€»ç»“
    print("\n" + "="*60)
    print("âš¡ æ€§èƒ½æµ‹è¯•æ€»ç»“")
    print("="*60)

    if "connection_tests" in results and results["connection_tests"]:
        print(f"\nâœ… è¿æ¥æ€§èƒ½: å¹³å‡{results['connection_tests']['average']:.3f}ç§’")

    if "fetch_tests" in results and results["fetch_tests"]:
        print(f"âœ… æŠ“å–æ€§èƒ½: å¹³å‡{results['fetch_tests']['average_total']:.3f}ç§’")

    if "stability_tests" in results and results["stability_tests"]:
        print(f"âœ… ç³»ç»Ÿåå: {results['stability_tests']['throughput']:.2f}é¡µé¢/ç§’")

    # ä¿å­˜ç»“æœ
    import json
    output_file = "tests/diagnostics/performance_benchmark_results.json"
    os.makedirs(os.path.dirname(output_file), exist_ok=True)

    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(results, f, indent=2, ensure_ascii=False)

    print(f"\nğŸ“ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {output_file}")

    return results

if __name__ == "__main__":
    run_performance_test()