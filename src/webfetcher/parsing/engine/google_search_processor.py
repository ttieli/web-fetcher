"""
Google Search Results Post-Processor
ä¸“é—¨å¤„ç†Googleæœç´¢ç»“æœçš„åå¤„ç†å™¨ï¼Œæå–ç»“æ„åŒ–æ•°æ®å¹¶æ ¼å¼åŒ–è¾“å‡º
"""

import re
import urllib.parse
from bs4 import BeautifulSoup, Tag
from typing import Dict, List, Optional, Any


class GoogleSearchProcessor:
    """Googleæœç´¢ç»“æœä¸“ç”¨å¤„ç†å™¨"""

    def __init__(self):
        """åˆå§‹åŒ–å¤„ç†å™¨"""
        pass

    def process_html(self, html: str, url: str) -> str:
        """
        å¤„ç†Googleæœç´¢ç»“æœHTMLï¼Œæå–ç»“æ„åŒ–æ•°æ®

        Args:
            html: åŸå§‹HTMLå†…å®¹
            url: é¡µé¢URL

        Returns:
            æ ¼å¼åŒ–åçš„Markdownå†…å®¹
        """
        soup = BeautifulSoup(html, 'html.parser')

        # æå–ç»“æ„åŒ–æ•°æ®
        results = {
            'knowledge_panel': self._extract_knowledge_panel(soup),
            'ai_overview': self._extract_ai_overview(soup),
            'images': self._extract_images(soup),
            'related_questions': self._extract_related_questions(soup),
            'web_results': self._extract_web_results(soup),
            'videos': self._extract_videos(soup),
            'news': self._extract_news(soup),
            'related_searches': self._extract_related_searches(soup),
        }

        # ç”Ÿæˆæ ¼å¼åŒ–çš„Markdown
        return self._format_markdown(results, url)

    def _extract_knowledge_panel(self, soup: BeautifulSoup) -> Optional[Dict[str, Any]]:
        """æå–çŸ¥è¯†é¢æ¿"""
        # GoogleçŸ¥è¯†é¢æ¿é€šå¸¸åœ¨å³ä¾§
        panel = soup.find('div', class_='kp-wholepage') or soup.find('div', {'data-attrid': True})

        if not panel:
            return None

        knowledge = {
            'title': None,
            'subtitle': None,
            'description': None,
            'facts': []
        }

        # æå–æ ‡é¢˜
        title_elem = panel.find('h2') or panel.find('span', class_='qrShPb')
        if title_elem:
            knowledge['title'] = title_elem.get_text(strip=True)

        # æå–æè¿°
        desc_elem = panel.find('div', class_='kno-rdesc') or panel.find('span', class_='hb8SAc')
        if desc_elem:
            knowledge['description'] = desc_elem.get_text(strip=True)

        return knowledge if knowledge['title'] else None

    def _extract_ai_overview(self, soup: BeautifulSoup) -> Optional[Dict[str, Any]]:
        """æå–AIæ¦‚è§ˆ"""
        # AIæ¦‚è§ˆé€šå¸¸åŒ…å«data-sgrdæˆ–ç‰¹å®šclass
        ai_container = soup.find('div', {'data-sgrd': True}) or soup.find('div', class_='ymu2H')

        if not ai_container:
            return None

        overview = {
            'summary': None,
            'points': []
        }

        # æå–æ‘˜è¦æ–‡æœ¬
        summary_elem = ai_container.find('div', recursive=False)
        if summary_elem:
            # è·å–ä¸»è¦æ–‡æœ¬ï¼Œæ’é™¤é“¾æ¥
            text_parts = []
            for elem in summary_elem.descendants:
                if isinstance(elem, str) and elem.strip():
                    text_parts.append(elem.strip())
            overview['summary'] = ' '.join(text_parts[:3]) if text_parts else None

        # æå–è¦ç‚¹åˆ—è¡¨
        for li in ai_container.find_all('li'):
            point_text = li.get_text(strip=True)
            if point_text and len(point_text) > 10:  # è¿‡æ»¤å¤ªçŸ­çš„å†…å®¹
                # æ¸…ç†URLç¼–ç çš„æ–‡æœ¬
                if '#:~:text=' not in point_text and 'http' not in point_text[:20]:
                    overview['points'].append(point_text[:200])  # é™åˆ¶é•¿åº¦

        return overview if (overview['summary'] or overview['points']) else None

    def _extract_related_questions(self, soup: BeautifulSoup) -> List[Dict[str, str]]:
        """æå–ç›¸å…³é—®é¢˜ (People Also Ask)"""
        questions = []

        # æŸ¥æ‰¾ç›¸å…³é—®é¢˜å®¹å™¨
        question_containers = soup.find_all('div', {'jsname': True, 'data-q': True})

        for container in question_containers[:5]:  # é™åˆ¶æ•°é‡
            question = container.get('data-q', '')
            if not question:
                continue

            # æå–ç­”æ¡ˆ
            answer_elem = container.find('div', {'data-attrid': True}) or container.find('span')
            answer = ''

            if answer_elem:
                answer = answer_elem.get_text(strip=True)
                # é™åˆ¶ç­”æ¡ˆé•¿åº¦
                if len(answer) > 300:
                    answer = answer[:297] + '...'

            if question:
                questions.append({
                    'question': question,
                    'answer': answer
                })

        return questions

    def _extract_web_results(self, soup: BeautifulSoup) -> List[Dict[str, Any]]:
        """æå–ç½‘é¡µæœç´¢ç»“æœ - ä½¿ç”¨æ›´é€šç”¨çš„æ–¹æ³•"""
        results = []

        # æ–¹æ³•1: æŸ¥æ‰¾æ‰€æœ‰åŒ…å«h3æ ‡é¢˜çš„é“¾æ¥ï¼ˆè¿™æ˜¯Googleæœç´¢ç»“æœçš„é€šç”¨ç‰¹å¾ï¼‰
        # æŸ¥æ‰¾æ‰€æœ‰h3æ ‡é¢˜
        h3_elements = soup.find_all('h3')

        for h3 in h3_elements[:20]:  # é™åˆ¶ç»“æœæ•°é‡
            result = {
                'title': None,
                'url': None,
                'snippet': None,
                'source': None
            }

            # è·å–æ ‡é¢˜æ–‡æœ¬
            result['title'] = h3.get_text(strip=True)

            # æŸ¥æ‰¾çˆ¶çº§é“¾æ¥
            parent_a = h3.find_parent('a')
            if parent_a and parent_a.get('href'):
                url = parent_a['href']
                # æ¸…ç†Googleé‡å®šå‘å’Œç›¸å¯¹è·¯å¾„
                if url.startswith('/url?'):
                    match = re.search(r'[?&]url=([^&]+)', url)
                    if match:
                        url = match.group(1)
                elif url.startswith('/search') or url.startswith('/webhp'):
                    # è·³è¿‡Googleå†…éƒ¨é“¾æ¥
                    continue
                result['url'] = url

            # å°è¯•åœ¨h3é™„è¿‘æ‰¾åˆ°æè¿°æ–‡æœ¬å’Œæ¥æº
            # è·å–h3æ‰€åœ¨çš„æœ€è¿‘å‡ å±‚çˆ¶çº§å®¹å™¨
            parent_div = h3.find_parent('div')

            if parent_div:
                # æå–citeï¼ˆæ¥æºURLæ˜¾ç¤ºï¼‰
                cite = parent_div.find('cite')
                if cite:
                    result['source'] = cite.get_text(strip=True)

                # æå–snippet - ä½¿ç”¨æ›´ç®€å•çš„æ–¹æ³•
                # è·å–çˆ¶å®¹å™¨çš„æ‰€æœ‰æ–‡æœ¬ï¼Œç„¶åç§»é™¤å·²çŸ¥çš„éƒ¨åˆ†
                full_text = parent_div.get_text(separator=' ', strip=True)

                # ç§»é™¤æ ‡é¢˜éƒ¨åˆ†
                text_without_title = full_text.replace(result['title'], '')

                # ç§»é™¤æ¥æºURLéƒ¨åˆ†ï¼ˆæ‰€æœ‰å‡ºç°ï¼‰
                if result.get('source'):
                    # ç§»é™¤æ‰€æœ‰å‡ºç°çš„source
                    while result['source'] in text_without_title:
                        text_without_title = text_without_title.replace(result['source'], '')

                # ç§»é™¤URLæœ¬èº«ï¼ˆå¦‚æœå®ƒå‡ºç°åœ¨æ–‡æœ¬ä¸­ï¼‰
                if result.get('url'):
                    text_without_title = text_without_title.replace(result['url'], '')

                # ç§»é™¤å¸¸è§çš„æ— ç”¨å‰ç¼€
                for prefix in ['Â·', 'â€º ', 'è½¬ä¸ºç®€ä½“ç½‘é¡µ', 'ç¿»è¯‘æ­¤é¡µ', '...']:
                    text_without_title = text_without_title.replace(prefix, '')

                # æ¸…ç†å¤šä½™ç©ºæ ¼å’Œç‰¹æ®Šå­—ç¬¦
                text_without_title = ' '.join(text_without_title.split())
                text_without_title = text_without_title.strip()

                # å¦‚æœæ¸…ç†åçš„æ–‡æœ¬é•¿åº¦åˆç†ï¼Œåˆ™ä½œä¸ºsnippet
                if 10 < len(text_without_title) < 1000:
                    # é™åˆ¶é•¿åº¦
                    result['snippet'] = text_without_title if len(text_without_title) <= 400 else text_without_title[:397] + '...'

            # åªæ·»åŠ æœ‰æ•ˆç»“æœï¼ˆå¿…é¡»æœ‰æ ‡é¢˜å’ŒURLï¼‰
            if result['title'] and result['url'] and len(result['title']) > 3:
                # å»é‡æ£€æŸ¥
                if not any(r['url'] == result['url'] for r in results):
                    results.append(result)

        return results

    def _extract_videos(self, soup: BeautifulSoup) -> List[Dict[str, Any]]:
        """æå–è§†é¢‘ç»“æœ"""
        videos = []

        # æŸ¥æ‰¾è§†é¢‘å®¹å™¨
        video_containers = soup.find_all('div', {'data-ved': True, 'data-md': True})

        for container in video_containers[:5]:
            video = {
                'title': None,
                'url': None,
                'source': None,
                'duration': None
            }

            # æå–æ ‡é¢˜
            title_elem = container.find('h3') or container.find('div', role='heading')
            if title_elem:
                video['title'] = title_elem.get_text(strip=True)

            # æå–URL
            link = container.find('a', href=True)
            if link:
                video['url'] = link['href']

            # æå–æ¥æº
            source_elem = container.find('cite') or container.find('span', class_='Zu0yb')
            if source_elem:
                video['source'] = source_elem.get_text(strip=True)

            if video['title'] and video['url']:
                videos.append(video)

        return videos

    def _extract_news(self, soup: BeautifulSoup) -> List[Dict[str, Any]]:
        """æå–æ–°é—»ç»“æœ"""
        news_items = []

        # æŸ¥æ‰¾æ–°é—»å®¹å™¨
        news_containers = soup.find_all('div', {'role': 'heading'})

        for container in news_containers[:5]:
            news = {
                'title': None,
                'url': None,
                'source': None,
                'time': None
            }

            # æå–æ ‡é¢˜
            news['title'] = container.get_text(strip=True)

            # æå–URL
            parent_div = container.find_parent('div')
            if parent_div:
                link = parent_div.find('a', href=True)
                if link:
                    news['url'] = link['href']

            # æå–æ—¶é—´
            time_elem = parent_div.find('span', class_='OSrXXb') if parent_div else None
            if time_elem:
                news['time'] = time_elem.get_text(strip=True)

            if news['title'] and news['url']:
                news_items.append(news)

        return news_items

    def _extract_images(self, soup: BeautifulSoup) -> List[Dict[str, Any]]:
        """æå–å›¾ç‰‡æœç´¢ç»“æœ"""
        images = []

        # æ–¹æ³•1: æŸ¥æ‰¾å¸¦æœ‰å›¾ç‰‡é“¾æ¥çš„å®¹å™¨ (/imgres? é“¾æ¥)
        img_containers = soup.find_all('a', href=lambda x: x and '/imgres?' in x if x else False)

        for container in img_containers[:6]:  # é™åˆ¶æ•°é‡
            image = {
                'title': None,
                'url': None,
                'source': None,
                'thumbnail': None
            }

            # æå–ç¼©ç•¥å›¾URL
            img_tag = container.find('img')
            if img_tag:
                # è·å–srcæˆ–data-src
                image['thumbnail'] = img_tag.get('src') or img_tag.get('data-src')
                # è·å–altä½œä¸ºæ ‡é¢˜
                image['title'] = img_tag.get('alt', '')

            # æå–åŸå›¾URL - ä»hrefå‚æ•°ä¸­è§£æ
            href = container.get('href', '')
            if '/imgres?' in href:
                # è§£æimgurlå‚æ•°
                params = urllib.parse.parse_qs(urllib.parse.urlparse(href).query)
                if 'imgurl' in params:
                    image['url'] = params['imgurl'][0]
                if 'imgrefurl' in params:
                    image['source'] = params['imgrefurl'][0]

            # åªæ·»åŠ æœ‰æ•ˆå›¾ç‰‡
            if image['thumbnail'] and (image['url'] or image['title']):
                images.append(image)

        # æ–¹æ³•2: å¦‚æœæ–¹æ³•1æ²¡æ‰¾åˆ°ï¼Œå°è¯•æŸ¥æ‰¾æ‰€æœ‰å›¾ç‰‡ï¼ˆä½†è¿‡æ»¤æ‰logoç­‰ï¼‰
        if not images:
            all_images = soup.find_all('img')
            for img in all_images[:10]:  # åªæŸ¥çœ‹å‰10ä¸ª
                src = img.get('src') or img.get('data-src')
                alt = img.get('alt', '')

                # è¿‡æ»¤æ¡ä»¶ï¼šæœ‰srcï¼Œæœ‰altï¼Œä¸æ˜¯logoæˆ–å›¾æ ‡
                if (src and alt and
                    len(alt) > 5 and  # altæ–‡æœ¬è¶³å¤Ÿé•¿
                    not any(x in src.lower() for x in ['logo', 'icon', 'sprite', 'button'])):

                    image = {
                        'title': alt,
                        'thumbnail': src,
                        'url': None,
                        'source': None
                    }
                    images.append(image)
                    if len(images) >= 3:  # é™åˆ¶æ•°é‡
                        break

        return images

    def _extract_related_searches(self, soup: BeautifulSoup) -> List[str]:
        """æå–ç›¸å…³æœç´¢å»ºè®® (ç”¨æˆ·è¿˜æœç´¢äº†)"""
        related = []
        seen = set()

        # æ–¹æ³•1: æŸ¥æ‰¾ç‰¹å®šçš„"ç›¸å…³æœç´¢"å®¹å™¨
        # Googleé€šå¸¸åœ¨é¡µé¢åº•éƒ¨æœ‰ "Searches related to..." æˆ–ç±»ä¼¼çš„å®¹å™¨
        related_section = soup.find('div', text=re.compile(r'(Related searches|ç›¸å…³æœç´¢|Searches related to)'))
        if related_section:
            parent = related_section.find_parent('div')
            if parent:
                # åœ¨è¿™ä¸ªå®¹å™¨ä¸­æŸ¥æ‰¾æ‰€æœ‰æœç´¢é“¾æ¥
                links = parent.find_all('a', href=lambda x: x and '/search?q=' in x if x else False)
                for link in links[:10]:
                    text = link.get_text(strip=True)
                    if text and 2 < len(text) < 50 and text not in seen:
                        related.append(text)
                        seen.add(text)

        # æ–¹æ³•2: æŸ¥æ‰¾æ‰€æœ‰æœç´¢é“¾æ¥ï¼Œè¿‡æ»¤å‡ºå¯èƒ½æ˜¯ç›¸å…³æœç´¢çš„
        if not related:
            search_links = soup.find_all('a', href=lambda x: x and '/search?q=' in x if x else False)

            for link in search_links:
                # è·å–é“¾æ¥æ–‡æœ¬
                text = link.get_text(strip=True)

                # è¿‡æ»¤æ¡ä»¶ï¼š
                # 1. æ–‡æœ¬ä¸ä¸ºç©º
                # 2. é•¿åº¦åˆç†ï¼ˆ2-50å­—ç¬¦ï¼‰
                # 3. ä¸åŒ…å«ç‰¹æ®Šå­—ç¬¦ã€URLæˆ–å¯¼èˆªæ–‡æœ¬
                # 4. æœªè§è¿‡
                # 5. ä¸æ˜¯å½“å‰æœç´¢é¡µé¢çš„ä¸»è¦å¯¼èˆªï¼ˆå¦‚"å…¨éƒ¨"ã€"å›¾ç‰‡"ç­‰ï¼‰
                nav_keywords = ['å…¨éƒ¨', 'å›¾ç‰‡', 'è§†é¢‘', 'æ–°é—»', 'è´­ç‰©', 'All', 'Images', 'Videos', 'News', 'Shopping']

                if (text and
                    2 < len(text) < 50 and
                    not any(char in text for char in ['http', 'â€º', '...', 'â–¸', '|']) and
                    text not in nav_keywords and
                    text not in seen):

                    # ä¼˜å…ˆä½¿ç”¨hrefä¸­çš„qå‚æ•°
                    href = link.get('href', '')
                    if '/search?q=' in href:
                        try:
                            parsed = urllib.parse.urlparse(href)
                            params = urllib.parse.parse_qs(parsed.query)

                            if 'q' in params:
                                query = params['q'][0]
                                if 2 < len(query) < 50 and query not in seen:
                                    related.append(query)
                                    seen.add(query)
                            elif text:
                                related.append(text)
                                seen.add(text)
                        except:
                            # è§£æå¤±è´¥ï¼Œä½¿ç”¨æ–‡æœ¬
                            if text and text not in seen:
                                related.append(text)
                                seen.add(text)

                    # é™åˆ¶æ•°é‡
                    if len(related) >= 10:
                        break

        return related[:10]  # è¿”å›æœ€å¤š10ä¸ª

    def _format_markdown(self, results: Dict[str, Any], url: str) -> str:
        """å°†æå–çš„ç»“æ„åŒ–æ•°æ®æ ¼å¼åŒ–ä¸ºMarkdown"""
        md_parts = []

        # çŸ¥è¯†é¢æ¿
        if results.get('knowledge_panel'):
            kp = results['knowledge_panel']
            md_parts.append('## ğŸ“Š çŸ¥è¯†é¢æ¿\n')
            if kp['title']:
                md_parts.append(f"**{kp['title']}**\n")
            if kp['description']:
                md_parts.append(f"{kp['description']}\n")
            md_parts.append('')

        # AIæ¦‚è§ˆ
        if results.get('ai_overview'):
            ai = results['ai_overview']
            md_parts.append('## ğŸ¤– AI æ¦‚è§ˆ\n')
            if ai['summary']:
                md_parts.append(f"{ai['summary']}\n")
            if ai['points']:
                md_parts.append('\n**å…³é”®ç‚¹ï¼š**\n')
                for point in ai['points'][:5]:
                    md_parts.append(f"- {point}")
            md_parts.append('')

        # å›¾ç‰‡ç»“æœ
        if results.get('images'):
            md_parts.append('## ğŸ“¸ å›¾ç‰‡\n')
            for i, img in enumerate(results['images'], 1):
                if img.get('title'):
                    md_parts.append(f"**{i}. {img['title']}**\n")
                else:
                    md_parts.append(f"**{i}. å›¾ç‰‡**\n")

                # æ˜¾ç¤ºç¼©ç•¥å›¾ï¼ˆMarkdownå›¾ç‰‡æ ¼å¼ï¼‰
                if img.get('thumbnail'):
                    title_text = img.get('title', 'å›¾ç‰‡')
                    md_parts.append(f"![{title_text}]({img['thumbnail']})\n")

                # æ˜¾ç¤ºæ¥æºé“¾æ¥
                if img.get('source'):
                    md_parts.append(f"- æ¥æº: <{img['source']}>\n")
                elif img.get('url'):
                    md_parts.append(f"- åŸå›¾: <{img['url']}>\n")

                md_parts.append('')

        # ç›¸å…³é—®é¢˜
        if results.get('related_questions'):
            md_parts.append('## â“ ç›¸å…³é—®é¢˜\n')
            for qa in results['related_questions']:
                md_parts.append(f"### {qa['question']}\n")
                if qa['answer']:
                    md_parts.append(f"{qa['answer']}\n")
            md_parts.append('')

        # ç½‘é¡µæœç´¢ç»“æœ
        if results.get('web_results'):
            md_parts.append('## ğŸ” æœç´¢ç»“æœ\n')
            for i, result in enumerate(results['web_results'], 1):
                md_parts.append(f"### {i}. {result['title']}\n")

                # æ ¼å¼åŒ–æ¥æºä¸ºè¶…é“¾æ¥
                if result.get('source'):
                    md_parts.append(f"**æ¥æº:** [{result['source']}]({result['url']})\n")

                # æ ¼å¼åŒ–é“¾æ¥ä¸ºè¶…é“¾æ¥ï¼ˆä½¿ç”¨å°–æ‹¬å·æ ¼å¼è‡ªåŠ¨ç”Ÿæˆé“¾æ¥ï¼‰
                md_parts.append(f"**é“¾æ¥:** <{result['url']}>\n")

                # åªæ˜¾ç¤ºæœ‰æ„ä¹‰çš„snippetï¼ˆè¿‡æ»¤æ‰é‡å¤å’Œæ— ç”¨å†…å®¹ï¼‰
                if result.get('snippet'):
                    snippet = result['snippet'].strip()
                    # è¿‡æ»¤å¤ªçŸ­æˆ–é‡å¤çš„snippet
                    if len(snippet) > 20 and snippet not in [result['title'], result.get('source', '')]:
                        md_parts.append(f"\n{snippet}\n")

                md_parts.append('')

        # è§†é¢‘ç»“æœ
        if results.get('videos'):
            md_parts.append('## ğŸ¬ è§†é¢‘\n')
            for video in results['videos']:
                md_parts.append(f"- **{video['title']}**")
                md_parts.append(f"  - é“¾æ¥: {video['url']}")
                if video.get('source'):
                    md_parts.append(f"  - æ¥æº: {video['source']}")
                md_parts.append('')

        # æ–°é—»ç»“æœ
        if results.get('news'):
            md_parts.append('## ğŸ“° æ–°é—»\n')
            for news in results['news']:
                md_parts.append(f"- **{news['title']}**")
                md_parts.append(f"  - {news['url']}")
                if news.get('time'):
                    md_parts.append(f"  - {news['time']}")
                md_parts.append('')

        # ç›¸å…³æœç´¢å»ºè®®
        if results.get('related_searches'):
            md_parts.append('## ğŸ”— ç›¸å…³æœç´¢\n')
            md_parts.append('**ç”¨æˆ·è¿˜æœç´¢äº†ï¼š**\n')
            for search in results['related_searches']:
                # åˆ›å»ºæœç´¢é“¾æ¥
                encoded = urllib.parse.quote(search)
                search_url = f"https://www.google.com/search?q={encoded}"
                md_parts.append(f"- [{search}]({search_url})")
            md_parts.append('')

        return '\n'.join(md_parts)


def process_google_search(html: str, url: str) -> str:
    """
    å¤„ç†Googleæœç´¢ç»“æœçš„å…¥å£å‡½æ•°

    Args:
        html: HTMLå†…å®¹
        url: é¡µé¢URL

    Returns:
        æ ¼å¼åŒ–åçš„Markdown
    """
    processor = GoogleSearchProcessor()
    return processor.process_html(html, url)
