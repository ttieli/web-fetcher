"""
Google Search Results Post-Processor
ä¸“é—¨å¤„ç†Googleæœç´¢ç»“æœçš„åå¤„ç†å™¨ï¼Œæå–ç»“æ„åŒ–æ•°æ®å¹¶æ ¼å¼åŒ–è¾“å‡º
"""

import re
from bs4 import BeautifulSoup, Tag
from typing import Dict, List, Optional, Any


class GoogleSearchProcessor:
    """Googleæœç´¢ç»“æœä¸“ç”¨å¤„ç†å™¨"""

    def __init__(self):
        """åˆå§‹åŒ–å¤„ç†å™¨"""
        pass

    def process_html(self, html: str, url: str) -> str:
        """
        å¤„ç†Googleæœç´¢ç»“æœHTMLï¼Œæå–ç»“æ„åŒ–æ•°æ®

        Args:
            html: åŸå§‹HTMLå†…å®¹
            url: é¡µé¢URL

        Returns:
            æ ¼å¼åŒ–åçš„Markdownå†…å®¹
        """
        soup = BeautifulSoup(html, 'html.parser')

        # æå–ç»“æ„åŒ–æ•°æ®
        results = {
            'knowledge_panel': self._extract_knowledge_panel(soup),
            'ai_overview': self._extract_ai_overview(soup),
            'related_questions': self._extract_related_questions(soup),
            'web_results': self._extract_web_results(soup),
            'videos': self._extract_videos(soup),
            'news': self._extract_news(soup),
        }

        # ç”Ÿæˆæ ¼å¼åŒ–çš„Markdown
        return self._format_markdown(results, url)

    def _extract_knowledge_panel(self, soup: BeautifulSoup) -> Optional[Dict[str, Any]]:
        """æå–çŸ¥è¯†é¢æ¿"""
        # GoogleçŸ¥è¯†é¢æ¿é€šå¸¸åœ¨å³ä¾§
        panel = soup.find('div', class_='kp-wholepage') or soup.find('div', {'data-attrid': True})

        if not panel:
            return None

        knowledge = {
            'title': None,
            'subtitle': None,
            'description': None,
            'facts': []
        }

        # æå–æ ‡é¢˜
        title_elem = panel.find('h2') or panel.find('span', class_='qrShPb')
        if title_elem:
            knowledge['title'] = title_elem.get_text(strip=True)

        # æå–æè¿°
        desc_elem = panel.find('div', class_='kno-rdesc') or panel.find('span', class_='hb8SAc')
        if desc_elem:
            knowledge['description'] = desc_elem.get_text(strip=True)

        return knowledge if knowledge['title'] else None

    def _extract_ai_overview(self, soup: BeautifulSoup) -> Optional[Dict[str, Any]]:
        """æå–AIæ¦‚è§ˆ"""
        # AIæ¦‚è§ˆé€šå¸¸åŒ…å«data-sgrdæˆ–ç‰¹å®šclass
        ai_container = soup.find('div', {'data-sgrd': True}) or soup.find('div', class_='ymu2H')

        if not ai_container:
            return None

        overview = {
            'summary': None,
            'points': []
        }

        # æå–æ‘˜è¦æ–‡æœ¬
        summary_elem = ai_container.find('div', recursive=False)
        if summary_elem:
            # è·å–ä¸»è¦æ–‡æœ¬ï¼Œæ’é™¤é“¾æ¥
            text_parts = []
            for elem in summary_elem.descendants:
                if isinstance(elem, str) and elem.strip():
                    text_parts.append(elem.strip())
            overview['summary'] = ' '.join(text_parts[:3]) if text_parts else None

        # æå–è¦ç‚¹åˆ—è¡¨
        for li in ai_container.find_all('li'):
            point_text = li.get_text(strip=True)
            if point_text and len(point_text) > 10:  # è¿‡æ»¤å¤ªçŸ­çš„å†…å®¹
                # æ¸…ç†URLç¼–ç çš„æ–‡æœ¬
                if '#:~:text=' not in point_text and 'http' not in point_text[:20]:
                    overview['points'].append(point_text[:200])  # é™åˆ¶é•¿åº¦

        return overview if (overview['summary'] or overview['points']) else None

    def _extract_related_questions(self, soup: BeautifulSoup) -> List[Dict[str, str]]:
        """æå–ç›¸å…³é—®é¢˜ (People Also Ask)"""
        questions = []

        # æŸ¥æ‰¾ç›¸å…³é—®é¢˜å®¹å™¨
        question_containers = soup.find_all('div', {'jsname': True, 'data-q': True})

        for container in question_containers[:5]:  # é™åˆ¶æ•°é‡
            question = container.get('data-q', '')
            if not question:
                continue

            # æå–ç­”æ¡ˆ
            answer_elem = container.find('div', {'data-attrid': True}) or container.find('span')
            answer = ''

            if answer_elem:
                answer = answer_elem.get_text(strip=True)
                # é™åˆ¶ç­”æ¡ˆé•¿åº¦
                if len(answer) > 300:
                    answer = answer[:297] + '...'

            if question:
                questions.append({
                    'question': question,
                    'answer': answer
                })

        return questions

    def _extract_web_results(self, soup: BeautifulSoup) -> List[Dict[str, Any]]:
        """æå–ç½‘é¡µæœç´¢ç»“æœ - ä½¿ç”¨æ›´é€šç”¨çš„æ–¹æ³•"""
        results = []

        # æ–¹æ³•1: æŸ¥æ‰¾æ‰€æœ‰åŒ…å«h3æ ‡é¢˜çš„é“¾æ¥ï¼ˆè¿™æ˜¯Googleæœç´¢ç»“æœçš„é€šç”¨ç‰¹å¾ï¼‰
        # æŸ¥æ‰¾æ‰€æœ‰h3æ ‡é¢˜
        h3_elements = soup.find_all('h3')

        for h3 in h3_elements[:20]:  # é™åˆ¶ç»“æœæ•°é‡
            result = {
                'title': None,
                'url': None,
                'snippet': None,
                'source': None
            }

            # è·å–æ ‡é¢˜æ–‡æœ¬
            result['title'] = h3.get_text(strip=True)

            # æŸ¥æ‰¾çˆ¶çº§é“¾æ¥
            parent_a = h3.find_parent('a')
            if parent_a and parent_a.get('href'):
                url = parent_a['href']
                # æ¸…ç†Googleé‡å®šå‘å’Œç›¸å¯¹è·¯å¾„
                if url.startswith('/url?'):
                    match = re.search(r'[?&]url=([^&]+)', url)
                    if match:
                        url = match.group(1)
                elif url.startswith('/search') or url.startswith('/webhp'):
                    # è·³è¿‡Googleå†…éƒ¨é“¾æ¥
                    continue
                result['url'] = url

            # å°è¯•åœ¨h3é™„è¿‘æ‰¾åˆ°æè¿°æ–‡æœ¬å’Œæ¥æº
            # è·å–h3æ‰€åœ¨çš„æœ€è¿‘å‡ å±‚çˆ¶çº§å®¹å™¨
            parent_div = h3.find_parent('div')

            if parent_div:
                # æå–citeï¼ˆæ¥æºURLæ˜¾ç¤ºï¼‰
                cite = parent_div.find('cite')
                if cite:
                    result['source'] = cite.get_text(strip=True)

                # æå–snippet - ä½¿ç”¨æ›´ç®€å•çš„æ–¹æ³•
                # è·å–çˆ¶å®¹å™¨çš„æ‰€æœ‰æ–‡æœ¬ï¼Œç„¶åç§»é™¤å·²çŸ¥çš„éƒ¨åˆ†
                full_text = parent_div.get_text(separator=' ', strip=True)

                # ç§»é™¤æ ‡é¢˜éƒ¨åˆ†
                text_without_title = full_text.replace(result['title'], '')

                # ç§»é™¤æ¥æºURLéƒ¨åˆ†ï¼ˆæ‰€æœ‰å‡ºç°ï¼‰
                if result.get('source'):
                    # ç§»é™¤æ‰€æœ‰å‡ºç°çš„source
                    while result['source'] in text_without_title:
                        text_without_title = text_without_title.replace(result['source'], '')

                # ç§»é™¤URLæœ¬èº«ï¼ˆå¦‚æœå®ƒå‡ºç°åœ¨æ–‡æœ¬ä¸­ï¼‰
                if result.get('url'):
                    text_without_title = text_without_title.replace(result['url'], '')

                # ç§»é™¤å¸¸è§çš„æ— ç”¨å‰ç¼€
                for prefix in ['Â·', 'â€º ', 'è½¬ä¸ºç®€ä½“ç½‘é¡µ', 'ç¿»è¯‘æ­¤é¡µ', '...']:
                    text_without_title = text_without_title.replace(prefix, '')

                # æ¸…ç†å¤šä½™ç©ºæ ¼å’Œç‰¹æ®Šå­—ç¬¦
                text_without_title = ' '.join(text_without_title.split())
                text_without_title = text_without_title.strip()

                # å¦‚æœæ¸…ç†åçš„æ–‡æœ¬é•¿åº¦åˆç†ï¼Œåˆ™ä½œä¸ºsnippet
                if 10 < len(text_without_title) < 1000:
                    # é™åˆ¶é•¿åº¦
                    result['snippet'] = text_without_title if len(text_without_title) <= 400 else text_without_title[:397] + '...'

            # åªæ·»åŠ æœ‰æ•ˆç»“æœï¼ˆå¿…é¡»æœ‰æ ‡é¢˜å’ŒURLï¼‰
            if result['title'] and result['url'] and len(result['title']) > 3:
                # å»é‡æ£€æŸ¥
                if not any(r['url'] == result['url'] for r in results):
                    results.append(result)

        return results

    def _extract_videos(self, soup: BeautifulSoup) -> List[Dict[str, Any]]:
        """æå–è§†é¢‘ç»“æœ"""
        videos = []

        # æŸ¥æ‰¾è§†é¢‘å®¹å™¨
        video_containers = soup.find_all('div', {'data-ved': True, 'data-md': True})

        for container in video_containers[:5]:
            video = {
                'title': None,
                'url': None,
                'source': None,
                'duration': None
            }

            # æå–æ ‡é¢˜
            title_elem = container.find('h3') or container.find('div', role='heading')
            if title_elem:
                video['title'] = title_elem.get_text(strip=True)

            # æå–URL
            link = container.find('a', href=True)
            if link:
                video['url'] = link['href']

            # æå–æ¥æº
            source_elem = container.find('cite') or container.find('span', class_='Zu0yb')
            if source_elem:
                video['source'] = source_elem.get_text(strip=True)

            if video['title'] and video['url']:
                videos.append(video)

        return videos

    def _extract_news(self, soup: BeautifulSoup) -> List[Dict[str, Any]]:
        """æå–æ–°é—»ç»“æœ"""
        news_items = []

        # æŸ¥æ‰¾æ–°é—»å®¹å™¨
        news_containers = soup.find_all('div', {'role': 'heading'})

        for container in news_containers[:5]:
            news = {
                'title': None,
                'url': None,
                'source': None,
                'time': None
            }

            # æå–æ ‡é¢˜
            news['title'] = container.get_text(strip=True)

            # æå–URL
            parent_div = container.find_parent('div')
            if parent_div:
                link = parent_div.find('a', href=True)
                if link:
                    news['url'] = link['href']

            # æå–æ—¶é—´
            time_elem = parent_div.find('span', class_='OSrXXb') if parent_div else None
            if time_elem:
                news['time'] = time_elem.get_text(strip=True)

            if news['title'] and news['url']:
                news_items.append(news)

        return news_items

    def _format_markdown(self, results: Dict[str, Any], url: str) -> str:
        """å°†æå–çš„ç»“æ„åŒ–æ•°æ®æ ¼å¼åŒ–ä¸ºMarkdown"""
        md_parts = []

        # çŸ¥è¯†é¢æ¿
        if results.get('knowledge_panel'):
            kp = results['knowledge_panel']
            md_parts.append('## ğŸ“Š çŸ¥è¯†é¢æ¿\n')
            if kp['title']:
                md_parts.append(f"**{kp['title']}**\n")
            if kp['description']:
                md_parts.append(f"{kp['description']}\n")
            md_parts.append('')

        # AIæ¦‚è§ˆ
        if results.get('ai_overview'):
            ai = results['ai_overview']
            md_parts.append('## ğŸ¤– AI æ¦‚è§ˆ\n')
            if ai['summary']:
                md_parts.append(f"{ai['summary']}\n")
            if ai['points']:
                md_parts.append('\n**å…³é”®ç‚¹ï¼š**\n')
                for point in ai['points'][:5]:
                    md_parts.append(f"- {point}")
            md_parts.append('')

        # ç›¸å…³é—®é¢˜
        if results.get('related_questions'):
            md_parts.append('## â“ ç›¸å…³é—®é¢˜\n')
            for qa in results['related_questions']:
                md_parts.append(f"### {qa['question']}\n")
                if qa['answer']:
                    md_parts.append(f"{qa['answer']}\n")
            md_parts.append('')

        # ç½‘é¡µæœç´¢ç»“æœ
        if results.get('web_results'):
            md_parts.append('## ğŸ” æœç´¢ç»“æœ\n')
            for i, result in enumerate(results['web_results'], 1):
                md_parts.append(f"### {i}. {result['title']}\n")

                # æ ¼å¼åŒ–æ¥æºä¸ºè¶…é“¾æ¥
                if result.get('source'):
                    md_parts.append(f"**æ¥æº:** [{result['source']}]({result['url']})\n")

                # æ ¼å¼åŒ–é“¾æ¥ä¸ºè¶…é“¾æ¥ï¼ˆä½¿ç”¨å°–æ‹¬å·æ ¼å¼è‡ªåŠ¨ç”Ÿæˆé“¾æ¥ï¼‰
                md_parts.append(f"**é“¾æ¥:** <{result['url']}>\n")

                # åªæ˜¾ç¤ºæœ‰æ„ä¹‰çš„snippetï¼ˆè¿‡æ»¤æ‰é‡å¤å’Œæ— ç”¨å†…å®¹ï¼‰
                if result.get('snippet'):
                    snippet = result['snippet'].strip()
                    # è¿‡æ»¤å¤ªçŸ­æˆ–é‡å¤çš„snippet
                    if len(snippet) > 20 and snippet not in [result['title'], result.get('source', '')]:
                        md_parts.append(f"\n{snippet}\n")

                md_parts.append('')

        # è§†é¢‘ç»“æœ
        if results.get('videos'):
            md_parts.append('## ğŸ¬ è§†é¢‘\n')
            for video in results['videos']:
                md_parts.append(f"- **{video['title']}**")
                md_parts.append(f"  - é“¾æ¥: {video['url']}")
                if video.get('source'):
                    md_parts.append(f"  - æ¥æº: {video['source']}")
                md_parts.append('')

        # æ–°é—»ç»“æœ
        if results.get('news'):
            md_parts.append('## ğŸ“° æ–°é—»\n')
            for news in results['news']:
                md_parts.append(f"- **{news['title']}**")
                md_parts.append(f"  - {news['url']}")
                if news.get('time'):
                    md_parts.append(f"  - {news['time']}")
                md_parts.append('')

        return '\n'.join(md_parts)


def process_google_search(html: str, url: str) -> str:
    """
    å¤„ç†Googleæœç´¢ç»“æœçš„å…¥å£å‡½æ•°

    Args:
        html: HTMLå†…å®¹
        url: é¡µé¢URL

    Returns:
        æ ¼å¼åŒ–åçš„Markdown
    """
    processor = GoogleSearchProcessor()
    return processor.process_html(html, url)
