# Task 4: Wikipedia Parser Optimization / ç»´åŸºç™¾ç§‘è§£æå™¨ä¼˜åŒ–

## Status / çŠ¶æ€
- ğŸ”„ **PENDING** / å¾…æ‰§è¡Œ

## Priority / ä¼˜å…ˆçº§
- P2 â€“ Important / é‡è¦ä¼˜å…ˆçº§

## Estimated Effort / é¢„è®¡å·¥æ—¶
- 6-8 hours / 6-8å°æ—¶
  - Template creation: 2-3h / æ¨¡æ¿åˆ›å»ºï¼š2-3å°æ—¶
  - Testing & validation: 2-3h / æµ‹è¯•ä¸éªŒè¯ï¼š2-3å°æ—¶
  - Documentation: 1-2h / æ–‡æ¡£ç¼–å†™ï¼š1-2å°æ—¶

---

## ğŸ“ Task Name / ä»»åŠ¡åç§°

Wikipedia Parser Optimization / ç»´åŸºç™¾ç§‘è§£æå™¨ä¼˜åŒ–

---

## Overview / æ¦‚è¿°

Currently, Wikipedia articles (zh.wikipedia.org) are parsed using the legacy generic parser, which produces poor-quality output with excessive navigation noise, missing infobox data, and leaked CSS code. This task migrates Wikipedia to the template-based parser system (Task-001 deliverable) to achieve clean, structured content extraction.

å½“å‰ç»´åŸºç™¾ç§‘æ–‡ç« ï¼ˆzh.wikipedia.orgï¼‰ä½¿ç”¨æ—§ç‰ˆé€šç”¨è§£æå™¨ï¼Œäº§ç”Ÿå¤§é‡å¯¼èˆªå™ªéŸ³ã€ç¼ºå¤±ä¿¡æ¯æ¡†æ•°æ®å’ŒCSSä»£ç æ³„æ¼çš„ä½è´¨é‡è¾“å‡ºã€‚æœ¬ä»»åŠ¡å°†ç»´åŸºç™¾ç§‘è¿ç§»åˆ°åŸºäºæ¨¡æ¿çš„è§£æå™¨ç³»ç»Ÿï¼ˆTask-001äº¤ä»˜ç‰©ï¼‰ï¼Œå®ç°å¹²å‡€ã€ç»“æ„åŒ–çš„å†…å®¹æå–ã€‚

### Current Problems / å½“å‰é—®é¢˜

**Test Article:** è‚å…ƒæ¢“ (https://zh.wikipedia.org/w/index.php?title=è‚å…ƒæ¢“)
**Output File:** 639 lines total

**Issue Analysis / é—®é¢˜åˆ†æ:**

1. **Massive Navigation Noise (Lines 1-124, ~19% of file) / å¤§é‡å¯¼èˆªå™ªéŸ³:**
   ```
   å¼€å…³ç›®å½•
   è‚å…ƒæ¢“
   7ç§è¯­è¨€
   ç¼–è¾‘é“¾æ¥
   å·¥å…·
   ç§»è‡³ä¾§æ 
   éšè—
   [... 100+ lines of UI elements ...]
   ```

2. **Missing Infobox Data / ç¼ºå¤±ä¿¡æ¯æ¡†æ•°æ®:**
   - No structured birth/death dates
   - No nationality, occupation metadata
   - Wikipedia infoboxes not extracted

3. **CSS Code Leakage (Line 192) / CSSä»£ç æ³„æ¼:**
   ```
   .mw-parser-output .refbegin{font-size:90%;margin-bottom:0.5em}...
   ```

4. **Poor Section Structure / ç« èŠ‚ç»“æ„ä¸ä½³:**
   - No proper markdown headers for "ç”Ÿå¹³", "å‚è€ƒæ–‡çŒ®"
   - Sections not clearly delimited

**Quality Metrics / è´¨é‡æŒ‡æ ‡:**
- **Content-to-Noise Ratio:** ~20% (120 noise lines / 639 total)
- **Missing Metadata:** Infobox fields, categories, interwiki links
- **Structural Issues:** No proper heading hierarchy

---

## ğŸ“‹ Requirements / å…·ä½“è¦æ±‚

### Must-Have Features / å¿…å¤‡åŠŸèƒ½

1. **Create Wikipedia-Specific Template / åˆ›å»ºç»´åŸºç™¾ç§‘ä¸“ç”¨æ¨¡æ¿**
   - Location: `parser_engine/templates/sites/wikipedia/zh_wikipedia.yaml`
   - Follow schema: `parser_engine/templates/schema.yaml`
   - Support Chinese variants: zh-cn, zh-tw, zh-hk, zh-sg

2. **Clean Content Extraction / å¹²å‡€å†…å®¹æå–**
   - Remove navigation elements (sidebar, language links, edit tools)
   - Remove footer links (privacy, disclaimers, categories UI)
   - Filter out CSS/JavaScript code
   - Preserve actual article content only

3. **Infobox Extraction / ä¿¡æ¯æ¡†æå–**
   - Extract `.infobox` table data
   - Parse birth/death dates, nationality, occupation
   - Format as structured metadata in markdown frontmatter

4. **Proper Section Structure / æ­£ç¡®ç« èŠ‚ç»“æ„**
   - Extract Wikipedia headings (h2.mw-headline, h3.mw-headline)
   - Convert to markdown headers (##, ###)
   - Preserve heading hierarchy

5. **Reference Handling / å‚è€ƒæ–‡çŒ®å¤„ç†**
   - Extract reference section (.references)
   - Clean reference formatting
   - Preserve citation links

6. **Chinese Variant Support / ä¸­æ–‡å˜ä½“æ”¯æŒ**
   - Handle variant parameter: `?variant=zh-cn`
   - Preserve selected variant in output

### Nice-to-Have Features / å¯é€‰åŠŸèƒ½

7. **Category Extraction / åˆ†ç±»æå–**
   - Extract article categories (footer categories)
   - Include in metadata

8. **Interwiki Links / è·¨è¯­è¨€é“¾æ¥**
   - Extract language links to other Wikipedia versions
   - Include in metadata

---

## ğŸ”§ Technical Approach / æŠ€æœ¯æ–¹æ¡ˆ

### Implementation Strategy / å®ç°ç­–ç•¥

#### 1. Template Creation / æ¨¡æ¿åˆ›å»º

**File:** `parser_engine/templates/sites/wikipedia/zh_wikipedia.yaml`

**Structure (based on wechat.yaml template):**

```yaml
name: "Wikipedia Chinese Articles"
version: "1.0.0"
domains:
  - "zh.wikipedia.org"
priority: 100  # High priority for exact domain match

selectors:
  # Title extraction
  title:
    - selector: "h1.firstHeading"
      strategy: "css"
    - selector: "title"
      strategy: "css"

  # Author (Wikipedia doesn't have single author, use "Wikipedia contributors")
  author:
    - selector: "meta[name='author']"
      strategy: "css"
      attribute: "content"
      default: "Wikipedia contributors"

  # Publish date (last modified)
  date:
    - selector: "#footer-info-lastmod"
      strategy: "css"
      transform: "extract_date"  # Extract date from "æœ¬é¡µé¢æœ€åä¿®è®¢äº..."

  # Main content (critical - must filter navigation)
  content:
    - selector: "#mw-content-text .mw-parser-output"
      strategy: "css"
      exclude:
        - ".mw-editsection"        # Edit links
        - "#toc"                    # Table of contents
        - ".navbox"                 # Navigation boxes
        - ".ambox"                  # Article messages
        - ".sistersitebox"          # Sister project links
        - "script"                  # JavaScript
        - "style"                   # CSS
        - ".mw-jump-link"           # Accessibility links
        - "#catlinks"               # Category links
        - ".printfooter"            # Print footer
        - ".mw-indicators"          # Page indicators

  # Infobox extraction
  metadata:
    infobox:
      - selector: ".infobox"
        strategy: "css"
        extract_table: true

    categories:
      - selector: "#mw-normal-catlinks ul li a"
        strategy: "css"
        extract_all: true

# Content filtering rules
filters:
  remove_patterns:
    - "^\\[ç¼–è¾‘\\]$"              # [ç¼–è¾‘] links
    - "^\\[æ¥æºè¯·æ±‚\\]$"          # Citation needed
    - "^ç›®å½•$"                   # Table of contents header

  css_classes_to_remove:
    - "mw-editsection"
    - "mw-jump"
    - "noprint"
    - "metadata"
```

#### 2. Routing Configuration / è·¯ç”±é…ç½®

**File:** `config/routing.yaml`

Add Wikipedia-specific rule:

```yaml
- name: "Wikipedia Chinese Articles"
  priority: 90
  pattern:
    domain: "zh.wikipedia.org"
  fetch:
    method: "urllib"  # Static content, no JS needed
  parse:
    parser: "template"
    template: "sites/wikipedia/zh_wikipedia.yaml"
```

#### 3. Parser Integration / è§£æå™¨é›†æˆ

**File:** `parsers_migrated.py`

No code changes needed - template system handles routing automatically.

#### 4. Testing Strategy / æµ‹è¯•ç­–ç•¥

**Create:** `tests/test_wikipedia_parser.py`

```python
def test_wikipedia_article_parsing():
    """Test Wikipedia article parsing with noise filtering"""
    url = "https://zh.wikipedia.org/w/index.php?title=è‚å…ƒæ¢“"
    result = fetch_and_parse(url)

    # Assert content quality
    assert "è‚å…ƒæ¢“" in result.title
    assert len(result.content) > 1000
    assert "å¼€å…³ç›®å½•" not in result.content  # No nav noise
    assert ".mw-parser-output" not in result.content  # No CSS

    # Assert infobox data
    assert result.metadata.get("infobox") is not None

def test_wikipedia_chinese_variants():
    """Test Chinese variant parameter handling"""
    url_cn = "...?variant=zh-cn"
    url_tw = "...?variant=zh-tw"
    # Test variant preservation
```

---

## In Scope / å·¥ä½œèŒƒå›´

### Deliverables / äº¤ä»˜ç‰©

1. âœ… **Wikipedia Template**
   - Location: `parser_engine/templates/sites/wikipedia/zh_wikipedia.yaml`
   - Validated against schema
   - Documented with inline comments

2. âœ… **Routing Rule**
   - Updated: `config/routing.yaml`
   - Wikipedia domain routing configured

3. âœ… **Test Suite**
   - File: `tests/test_wikipedia_parser.py`
   - Test cases:
     - Content extraction (no nav noise)
     - Infobox parsing
     - Chinese variant support
     - Reference section handling
     - Edge cases (stub articles, disambiguation pages)

4. âœ… **Documentation**
   - README: `parser_engine/templates/sites/wikipedia/README.md`
   - Usage examples
   - Known limitations

5. âœ… **Quality Validation**
   - Before/after comparison report
   - Content completeness metrics
   - Parsing speed benchmarks

---

## Out of Scope / éèŒƒå›´äº‹é¡¹

The following items are explicitly excluded from this task:

1. âŒ **Other Wikipedia Language Versions**
   - Only Chinese Wikipedia (zh.wikipedia.org)
   - English/other languages: future task

2. âŒ **Wikipedia-Specific Content Types**
   - Disambiguation pages (special handling needed)
   - List articles (different structure)
   - Portal pages

3. âŒ **Advanced MediaWiki Features**
   - Template transclusion rendering
   - Math formula parsing
   - Gallery/media handling

4. âŒ **Historical Revisions**
   - Only current version parsing
   - No diff/revision comparison

5. âŒ **Wikidata Integration**
   - No Wikidata API calls
   - No structured data from Wikidata

---

## Dependencies / ä¾èµ–

### Required / å¿…éœ€

1. âœ… **parser_engine/ Infrastructure**
   - Confirmed active (Task-3 analysis)
   - Template loader working
   - Validation tools available

2. âœ… **Template System**
   - Task-001 deliverable
   - Schema validation tools
   - Template generator CLI

3. âœ… **Routing System**
   - Config-driven routing (config/routing.yaml)
   - Template-based parser integration

### Optional / å¯é€‰

4. âš ï¸ **Existing Templates as Reference**
   - wechat.yaml - WeChat article structure
   - xiaohongshu.yaml - XiaoHongShu content pattern
   - Use as architectural examples

---

## Risks & Mitigations / é£é™©ä¸ç¼“è§£

### Risk 1: Wikipedia HTML Structure Changes / é£é™©1ï¼šç»´åŸºç™¾ç§‘HTMLç»“æ„å˜æ›´

**Impact:** Template selectors may break if Wikipedia updates their HTML
**Probability:** Low (Wikipedia has stable structure)
**Mitigation:**
- Use multiple fallback selectors for critical fields
- Monitor Wikipedia technical changes
- Include version notes in template

### Risk 2: Chinese Variant Handling Complexity / é£é™©2ï¼šä¸­æ–‡å˜ä½“å¤„ç†å¤æ‚æ€§

**Impact:** Different variants may render different content
**Probability:** Medium
**Mitigation:**
- Test all major variants (zh-cn, zh-tw, zh-hk)
- Document variant-specific quirks
- Use variant-agnostic selectors where possible

### Risk 3: Infobox Parsing Variability / é£é™©3ï¼šä¿¡æ¯æ¡†è§£æå˜åŒ–æ€§

**Impact:** Different article types have different infobox structures
**Probability:** High (infoboxes vary greatly)
**Mitigation:**
- Use flexible table extraction
- Handle missing infobox gracefully
- Test multiple article types (person, place, event)

### Risk 4: Performance with Large Articles / é£é™©4ï¼šå¤§å‹æ–‡ç« æ€§èƒ½é—®é¢˜

**Impact:** Very long articles may slow parsing
**Probability:** Low
**Mitigation:**
- Benchmark with large articles (>50KB)
- Optimize selector specificity
- Set reasonable timeout limits

---

## âœ… Acceptance Criteria / éªŒæ”¶æ ‡å‡†

### Functional Criteria / åŠŸèƒ½æ ‡å‡†

- [ ] **Template Created and Valid**
  - zh_wikipedia.yaml exists at correct location
  - Passes schema validation
  - All required selectors defined

- [ ] **Content Quality Improved**
  - Navigation noise removed (<5% of output)
  - No CSS/JavaScript code in output
  - Main article content extracted cleanly

- [ ] **Infobox Extraction Working**
  - Infobox data captured in metadata
  - At least 3 fields extracted (for articles with infoboxes)
  - Graceful handling when infobox absent

- [ ] **Section Structure Preserved**
  - Wikipedia headings converted to markdown (##, ###)
  - Heading hierarchy maintained
  - Section content properly associated

- [ ] **Chinese Variant Support**
  - Variant parameter preserved in requests
  - Content rendered in selected variant
  - Tested: zh-cn, zh-tw, zh-hk

- [ ] **References Section Clean**
  - Reference links extracted
  - Citation formatting preserved
  - No duplicate footnote markers

### Testing Criteria / æµ‹è¯•æ ‡å‡†

- [ ] **All Tests Passing**
  - test_wikipedia_parser.py: 100% pass rate
  - At least 5 test cases covering:
    - Content extraction
    - Infobox parsing
    - Variant handling
    - Reference section
    - Edge cases

- [ ] **Regression Testing**
  - Existing tests still pass (no breaking changes)
  - Generic parser unaffected
  - Other templates unaffected

### Quality Criteria / è´¨é‡æ ‡å‡†

- [ ] **Content-to-Noise Ratio: >95%**
  - Measured: actual content / total output
  - Baseline: ~20% â†’ Target: >95%

- [ ] **Parsing Speed: <3s**
  - Typical article (20-50KB) parses in <3 seconds
  - No significant slowdown vs generic parser

- [ ] **Documentation Complete**
  - Template inline comments explain selectors
  - README with usage examples
  - Known limitations documented

---

## Success Metrics / æˆåŠŸæŒ‡æ ‡

### Before/After Comparison / å‰åå¯¹æ¯”

**Test Article:** è‚å…ƒæ¢“ (https://zh.wikipedia.org/w/index.php?title=è‚å…ƒæ¢“)

| Metric / æŒ‡æ ‡ | Before / ä¼˜åŒ–å‰ | Target / ç›®æ ‡ |
|--------------|----------------|--------------|
| Output Lines | 639 | <250 |
| Nav Noise Lines | 120 (19%) | <10 (<4%) |
| Content-to-Noise Ratio | ~20% | >95% |
| Infobox Fields Extracted | 0 | â‰¥3 |
| CSS Code Leaks | Yes (line 192) | None |
| Parse Time | 1.6s | <3s |
| Section Headers | Mixed | Clean ## hierarchy |

### Quality Gates / è´¨é‡å…³å¡

**Must achieve all:**
1. âœ… Content-to-noise ratio >95%
2. âœ… Zero CSS/JS code in output
3. âœ… Infobox extraction working (when present)
4. âœ… All tests passing
5. âœ… Documentation complete

**Nice-to-have:**
6. â­ Parse time <2s
7. â­ Category extraction working
8. â­ Interwiki links captured

---

## Milestones / é‡Œç¨‹ç¢‘

### Phase 1: Template Creation (2-3 hours) / é˜¶æ®µ1ï¼šæ¨¡æ¿åˆ›å»º

**Duration:** 2-3h

**Tasks:**
1. Create directory: `parser_engine/templates/sites/wikipedia/`
2. Analyze Wikipedia HTML structure (inspect article page)
3. Identify CSS selectors for:
   - Main content (.mw-parser-output)
   - Title (h1.firstHeading)
   - Infobox (.infobox)
   - Headings (.mw-headline)
   - References (.references)
4. Create zh_wikipedia.yaml template
5. Validate against schema: `python -m parser_engine.tools.validators.schema_validator`

**Deliverable:** Validated Wikipedia template

---

### Phase 2: Routing & Integration (1-2 hours) / é˜¶æ®µ2ï¼šè·¯ç”±ä¸é›†æˆ

**Duration:** 1-2h

**Tasks:**
1. Update `config/routing.yaml` with Wikipedia rule
2. Test routing decision: `wf https://zh.wikipedia.org/... --debug`
3. Verify template loading in parsers_migrated.py
4. Test basic parsing with new template
5. Compare output quality with baseline

**Deliverable:** Integrated and routing properly

---

### Phase 3: Testing & Validation (2-3 hours) / é˜¶æ®µ3ï¼šæµ‹è¯•ä¸éªŒè¯

**Duration:** 2-3h

**Tasks:**
1. Create `tests/test_wikipedia_parser.py`
2. Write test cases:
   - `test_content_extraction_no_noise()`
   - `test_infobox_parsing()`
   - `test_chinese_variant_support()`
   - `test_reference_section_clean()`
   - `test_edge_case_stub_article()`
3. Run full test suite: `pytest tests/ -v`
4. Measure quality metrics (before/after)
5. Create comparison report

**Deliverable:** Passing test suite + quality report

---

### Phase 4: Documentation & Finalization (1-2 hours) / é˜¶æ®µ4ï¼šæ–‡æ¡£ä¸å®Œå–„

**Duration:** 1-2h

**Tasks:**
1. Create `parser_engine/templates/sites/wikipedia/README.md`
2. Add usage examples:
   ```bash
   wf "https://zh.wikipedia.org/wiki/è‚å…ƒæ¢“"
   wf "https://zh.wikipedia.org/wiki/ä¸­å›½?variant=zh-cn"
   ```
3. Document known limitations
4. Add inline comments to template
5. Update TASKS/README.md with completion status
6. Create completion report

**Deliverable:** Complete documentation

---

## Notes / å¤‡æ³¨

### Reference Templates / å‚è€ƒæ¨¡æ¿

Study existing templates for architectural patterns:
- `parser_engine/templates/sites/wechat/wechat.yaml` - Rich selector examples
- `parser_engine/templates/sites/xiaohongshu/xiaohongshu.yaml` - Content filtering
- `parser_engine/templates/schema.yaml` - Template structure

### Wikipedia Markup Specifics / ç»´åŸºç™¾ç§‘æ ‡è®°ç‰¹æ€§

**Key CSS Classes to Handle:**
- `.mw-parser-output` - Main content wrapper
- `.infobox` - Infobox table
- `.mw-headline` - Section headings
- `.references` - Reference list
- `.mw-editsection` - Edit links (remove)
- `.navbox` - Navigation boxes (remove)
- `.catlinks` - Category links

**MediaWiki Structure:**
```html
<div id="mw-content-text" class="mw-body-content">
  <div class="mw-parser-output">
    <table class="infobox">...</table>
    <p>Article content...</p>
    <h2><span class="mw-headline">Section</span></h2>
    <div class="reflist references">...</div>
  </div>
</div>
```

### Testing URLs / æµ‹è¯•URL

Use these URLs for comprehensive testing:

1. **Standard Biography:** https://zh.wikipedia.org/wiki/è‚å…ƒæ¢“
2. **With Infobox:** https://zh.wikipedia.org/wiki/ä¸­åäººæ°‘å…±å’Œå›½
3. **Stub Article (short):** https://zh.wikipedia.org/wiki/é»„é¾™é•‡_(åŒ—äº¬å¸‚)
4. **Long Article:** https://zh.wikipedia.org/wiki/ç¬¬äºŒæ¬¡ä¸–ç•Œå¤§æˆ˜
5. **Chinese Variants:**
   - Simplified: `?variant=zh-cn`
   - Traditional: `?variant=zh-tw`
   - Hong Kong: `?variant=zh-hk`

---

## Version History / ç‰ˆæœ¬å†å²

- **v1.0 (2025-10-10):** Initial task creation
  - Analysis of current Wikipedia parsing issues
  - Requirements and technical approach defined
  - Estimated 6-8 hours for complete implementation

---

**Created By / åˆ›å»ºè€…:** Archy Principle Architect
**Date / æ—¥æœŸ:** 2025-10-10
**Status / çŠ¶æ€:** Ready for execution / å‡†å¤‡æ‰§è¡Œ
**Prerequisite / å‰ç½®æ¡ä»¶:** Task-001 (Parser Template Creator) completed âœ…
