# Task-008: Enhanced Multi-Page and Whole-Site Crawling
# Task-008ï¼šå¢å¼ºçš„å¤šé¡µé¢ä¸æ•´ç«™çˆ¬å–åŠŸèƒ½

**Priority / ä¼˜å…ˆçº§:** P2 (Important / é‡è¦)
**Status / çŠ¶æ€:** PENDING / å¾…åŠ
**Created / åˆ›å»ºæ—¥æœŸ:** 2025-10-10
**Revised / ä¿®è®¢æ—¥æœŸ:** 2025-10-10 (Removed robots.txt - personal use only)
**Estimated Effort / é¢„è®¡å·¥æ—¶:** 14-19 hours / 14-19å°æ—¶

---

## Executive Summary / æ‰§è¡Œæ‘˜è¦

Enhance the existing `wf --site` command with improved multi-page crawling capabilities, better configuration options, and structured output formats, while maintaining backward compatibility with the current CLI interface.

å¢å¼ºç°æœ‰çš„ `wf --site` å‘½ä»¤ï¼Œæä¾›æ”¹è¿›çš„å¤šé¡µé¢çˆ¬å–èƒ½åŠ›ã€æ›´å¥½çš„é…ç½®é€‰é¡¹å’Œç»“æ„åŒ–è¾“å‡ºæ ¼å¼ï¼ŒåŒæ—¶ä¿æŒä¸å½“å‰CLIç•Œé¢çš„å‘åå…¼å®¹æ€§ã€‚

---

## Background / èƒŒæ™¯

### Current Implementation Analysis / å½“å‰å®ç°åˆ†æ

**Existing Capabilities / ç°æœ‰èƒ½åŠ›:**

1. **CLI Interface / CLIæ¥å£** (`wf.py:426-446`)
   ```bash
   wf site <URL> [è¾“å‡ºç›®å½•] [--max-pages N]
   ```
   - Passes to webfetcher: `--crawl-site --max-crawl-depth 5 --follow-pagination`
   - Supports output directory specification
   - Accepts additional arguments passthrough

2. **Core Crawling Engine / æ ¸å¿ƒçˆ¬å–å¼•æ“** (`webfetcher.py:3153-3360`)
   - **Algorithm / ç®—æ³•:** BFS (Breadth-First Search) traversal
   - **Deduplication / å»é‡:** Normalized URL comparison
   - **Depth Control / æ·±åº¦æ§åˆ¶:** Max depth limit (default: 10, wf.py uses: 5)
   - **Page Limit / é¡µé¢é™åˆ¶:** Max pages (default: 1000)
   - **Rate Limiting / é€Ÿç‡é™åˆ¶:** Configurable delay (default: 0.5s)
   - **Progress Reporting / è¿›åº¦æŠ¥å‘Š:** Real-time progress line
   - **Link Filtering / é“¾æ¥è¿‡æ»¤:** Documentation URL pre-filtering
   - **Memory Optimization / å†…å­˜ä¼˜åŒ–:** Batch processing mode

3. **Advanced Features / é«˜çº§ç‰¹æ€§:**
   - **Category-First Strategy / åˆ†ç±»ä¼˜å…ˆç­–ç•¥** (`crawl_site_by_categories()`)
     - Special handling for government sites
     - Extracts site categories and crawls by category
   - **Pagination Support / åˆ†é¡µæ”¯æŒ** (`process_pagination()`)
     - Follows Docusaurus-style pagination links
     - Prevents circular pagination loops

### Identified Issues / å‘ç°çš„é—®é¢˜

**Critical Bugs / å…³é”®ç¼ºé™·:**
1. âŒ **Bug #1**: `--follow-pagination` flag passed by wf.py **doesn't exist** in webfetcher.py
   - Current: `wf site` command fails with "unrecognized arguments: --follow-pagination"
   - Impact: Pagination feature is **completely broken**

**Limitations / å±€é™æ€§:**
1. ğŸ”§ **Fixed Parameters / å›ºå®šå‚æ•°:** Depth hardcoded to 5 in wf.py (should be configurable)
2. ğŸš« **No Sitemap Support / ä¸æ”¯æŒsitemap.xml:** Misses efficient site discovery
3. ğŸ”— **Limited URL Filtering / æœ‰é™çš„URLè¿‡æ»¤:** Only supports documentation URL filter
4. ğŸŒ **No Domain Boundaries / æ— åŸŸåè¾¹ç•Œ:** Doesn't enforce same-domain crawling
5. ğŸ’¾ **No Resume Capability / æ— æ¢å¤èƒ½åŠ›:** Can't resume interrupted crawls
6. ğŸ“Š **Limited Output Formats / æœ‰é™çš„è¾“å‡ºæ ¼å¼:** Only dumps files, no structured index

---

## Objectives / ç›®æ ‡

### Primary Goals / ä¸»è¦ç›®æ ‡

1. **Fix Critical Bugs / ä¿®å¤å…³é”®ç¼ºé™·**
   - Implement missing `--follow-pagination` flag
   - Make `wf site` command fully functional

2. **Enhance Multi-Page Crawling / å¢å¼ºå¤šé¡µé¢çˆ¬å–**
   - Support targeted page range crawling (e.g., crawl pages 1-50)
   - Support URL pattern-based crawling (e.g., only /docs/* paths)
   - Follow pagination links intelligently

3. **Improve Whole-Site Crawling / æ”¹è¿›æ•´ç«™çˆ¬å–**
   - Add sitemap.xml discovery and parsing
   - Enforce domain boundary controls
   - Improve link discovery algorithms
   - **Note / æ³¨æ„:** No robots.txt support (personal use tool, not for production crawling)

4. **Enhance Configuration / å¢å¼ºé…ç½®**
   - Expose all crawl parameters via CLI
   - Support crawl configuration files
   - Add preset strategies (documentation, blog, e-commerce, etc.)

5. **Structured Output / ç»“æ„åŒ–è¾“å‡º**
   - Generate JSON index of crawled pages
   - Generate CSV reports with metadata
   - Create site map visualization

6. **Robustness / é²æ£’æ€§**
   - Add resume capability for interrupted crawls
   - Better error handling and recovery
   - Crawl state persistence

---

## Detailed Requirements / è¯¦ç»†éœ€æ±‚

### Phase 1: Bug Fixes and Parameter Exposure (4-6 hours) / é˜¶æ®µ1ï¼šç¼ºé™·ä¿®å¤ä¸å‚æ•°æš´éœ²

**1.1 Fix --follow-pagination Bug / ä¿®å¤åˆ†é¡µæ ‡å¿—ç¼ºé™·**
- [ ] Add `--follow-pagination` flag to webfetcher.py argparser
- [ ] Integrate with existing `process_pagination()` function
- [ ] Add tests to verify pagination following works

**1.2 Expose Crawl Parameters in wf.py / åœ¨wf.pyä¸­æš´éœ²çˆ¬å–å‚æ•°**
```bash
wf site <URL> [output_dir] [options]

Options:
  --max-pages N           # Maximum pages to crawl (default: 100, max: 1000)
  --max-depth N           # Maximum crawl depth (default: 5, max: 10)
  --delay SECONDS         # Delay between requests (default: 0.5)
  --follow-pagination     # Follow pagination links
  --same-domain-only      # Only crawl same domain (default: true)
```

**1.3 Update wf.py Site Handler / æ›´æ–°wf.pyç«™ç‚¹å¤„ç†å™¨**
- Remove hardcoded parameters
- Pass through user-specified options
- Maintain backward compatibility

**Acceptance Criteria / éªŒæ”¶æ ‡å‡†:**
- âœ… `wf site` command works without errors
- âœ… All parameters can be configured via CLI
- âœ… Pagination following works correctly
- âœ… Backward compatibility maintained (existing commands still work)

---

### Phase 2: Sitemap Support (3-4 hours) / é˜¶æ®µ2ï¼šSitemapæ”¯æŒ

**Note / è¯´æ˜:** Robots.txt support has been removed as this tool is for personal use only, not production web crawling. / å·²ç§»é™¤robots.txtæ”¯æŒï¼Œå› ä¸ºæ­¤å·¥å…·ä»…ç”¨äºä¸ªäººç”¨é€”ï¼Œéç”Ÿäº§ç¯å¢ƒçˆ¬è™«ã€‚

**2.1 Sitemap.xml Parser / Sitemap.xmlè§£æå™¨**
- [ ] Implement sitemap.xml discovery (check /sitemap.xml, /sitemap_index.xml)
- [ ] Parse sitemap.xml and sitemap index files
- [ ] Extract URLs with priorities and lastmod dates
- [ ] Handle gzipped sitemaps (sitemap.xml.gz)
- [ ] Add `--use-sitemap` flag to enable sitemap-first crawling

**2.2 Sitemap-First Crawling Strategy / Sitemapä¼˜å…ˆçˆ¬å–ç­–ç•¥**
```python
def crawl_from_sitemap(start_url, **kwargs):
    """Crawl using sitemap.xml as the primary URL source."""
    # 1. Discover sitemap
    sitemap_urls = discover_sitemaps(start_url)
    if not sitemap_urls:
        logging.info("No sitemap found, falling back to BFS")
        return crawl_site(start_url, **kwargs)

    # 2. Parse sitemap and extract URLs
    all_urls = []
    for sitemap_url in sitemap_urls:
        urls = parse_sitemap(sitemap_url)
        all_urls.extend(urls)

    # 3. Filter and prioritize URLs
    filtered_urls = filter_urls_by_pattern(all_urls, **kwargs)
    sorted_urls = sort_by_priority_and_lastmod(filtered_urls)

    # 4. Crawl URLs from sitemap
    return crawl_url_list(sorted_urls[:kwargs.get('max_pages', 1000)])
```

**2.3 Integration / é›†æˆ**
- [ ] Add sitemap discovery to existing `crawl_site()` function
- [ ] Create separate `crawl_from_sitemap()` function
- [ ] Add CLI flag `--use-sitemap` to wf.py
- [ ] Fall back to BFS if sitemap not found

**Acceptance Criteria / éªŒæ”¶æ ‡å‡†:**
- âœ… Discovers and parses sitemap.xml successfully
- âœ… Can crawl from sitemap when available
- âœ… Handles sitemap index files (multiple sitemaps)
- âœ… Falls back to BFS crawling if no sitemap found
- âœ… `--use-sitemap` flag works correctly

---

### Phase 3: Advanced Crawling Features (4-6 hours) / é˜¶æ®µ3ï¼šé«˜çº§çˆ¬å–ç‰¹æ€§

**3.1 URL Pattern Filtering / URLæ¨¡å¼è¿‡æ»¤**
```bash
wf site <URL> --include-pattern "/docs/*" --exclude-pattern "*/archive/*"
```
- [ ] Support glob patterns for URL filtering
- [ ] Support regex patterns (with `--regex` flag)
- [ ] Multiple include/exclude patterns

**3.2 Domain Boundary Control / åŸŸåè¾¹ç•Œæ§åˆ¶**
```bash
wf site <URL> --same-domain-only    # Default: only crawl same domain
wf site <URL> --allow-subdomains    # Allow subdomains (e.g., blog.example.com)
wf site <URL> --follow-external     # Follow external links (with limits)
```

**3.3 Link Discovery Improvements / é“¾æ¥å‘ç°æ”¹è¿›**
- [ ] Extract links from `<a>`, `<link>`, `<iframe>` tags
- [ ] Handle JavaScript-rendered links (via Selenium mode)
- [ ] Support canonical URL deduplication
- [ ] Handle redirect chains

**3.4 Crawl Strategies / çˆ¬å–ç­–ç•¥**
```bash
wf site <URL> --strategy documentation  # Optimized for docs sites
wf site <URL> --strategy blog           # Follow blog pagination
wf site <URL> --strategy e-commerce     # Product listing crawling
wf site <URL> --strategy news           # News article crawling
```

**Acceptance Criteria / éªŒæ”¶æ ‡å‡†:**
- âœ… URL pattern filtering works correctly
- âœ… Domain boundaries are enforced
- âœ… Crawl strategies optimize for different site types
- âœ… Link discovery handles edge cases (redirects, canonical, etc.)

---

### Phase 4: Structured Output and Reporting (3-4 hours) / é˜¶æ®µ4ï¼šç»“æ„åŒ–è¾“å‡ºä¸æŠ¥å‘Š

**4.1 JSON Index Generation / JSONç´¢å¼•ç”Ÿæˆ**
```json
{
  "crawl_metadata": {
    "start_url": "https://example.com",
    "timestamp": "2025-10-10T12:00:00Z",
    "total_pages": 150,
    "total_size_bytes": 15000000,
    "duration_seconds": 120
  },
  "pages": [
    {
      "url": "https://example.com/page1",
      "depth": 0,
      "status": "success",
      "size_bytes": 50000,
      "links_found": 25,
      "fetch_time_ms": 250
    }
  ]
}
```

**4.2 CSV Report Generation / CSVæŠ¥å‘Šç”Ÿæˆ**
```csv
url,depth,status,size_bytes,links_found,fetch_time_ms
https://example.com,0,success,50000,25,250
```

**4.3 Site Map Visualization / ç«™ç‚¹åœ°å›¾å¯è§†åŒ–**
- [ ] Generate Mermaid diagram of site structure
- [ ] Show depth levels and link relationships
- [ ] Highlight entry points and dead ends

**4.4 CLI Integration / CLIé›†æˆ**
```bash
wf site <URL> --output-json crawl_index.json
wf site <URL> --output-csv crawl_report.csv
wf site <URL> --output-sitemap sitemap.md
```

**Acceptance Criteria / éªŒæ”¶æ ‡å‡†:**
- âœ… JSON index contains complete crawl metadata
- âœ… CSV report is importable into spreadsheets
- âœ… Site map visualization is readable and useful
- âœ… All output formats can be enabled simultaneously

---

### Phase 5: Resume Capability and State Persistence (3-4 hours) / é˜¶æ®µ5ï¼šæ¢å¤èƒ½åŠ›ä¸çŠ¶æ€æŒä¹…åŒ–

**5.1 Crawl State Persistence / çˆ¬å–çŠ¶æ€æŒä¹…åŒ–**
```python
# .crawl_state.json
{
  "start_url": "https://example.com",
  "visited": ["url1", "url2", ...],
  "queue": [["url3", 2], ["url4", 3], ...],
  "statistics": {...},
  "timestamp": "2025-10-10T12:00:00Z"
}
```

**5.2 Resume Logic / æ¢å¤é€»è¾‘**
```bash
wf site <URL> --resume    # Resume from last state
wf site <URL> --resume-from crawl_state.json
```
- [ ] Save state periodically (every 50 pages or 5 minutes)
- [ ] Load state on resume
- [ ] Handle state version compatibility
- [ ] Clean up state file on successful completion

**5.3 Crash Recovery / å´©æºƒæ¢å¤**
- [ ] Auto-save state before each request
- [ ] Detect incomplete state on startup
- [ ] Offer to resume automatically

**Acceptance Criteria / éªŒæ”¶æ ‡å‡†:**
- âœ… Crawl can be interrupted and resumed
- âœ… No duplicate fetching after resume
- âœ… State file is human-readable and debuggable
- âœ… State cleanup happens automatically

---

## Technical Architecture / æŠ€æœ¯æ¶æ„

### Component Design / ç»„ä»¶è®¾è®¡

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    wf.py (CLI Layer)                     â”‚
â”‚  - Parse user commands                                   â”‚
â”‚  - Expose all crawl parameters                           â”‚
â”‚  - Generate command for webfetcher.py                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              webfetcher.py (Orchestration)               â”‚
â”‚  - Parse CLI arguments                                   â”‚
â”‚  - Initialize crawl configuration                        â”‚
â”‚  - Call appropriate crawler                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           crawler/ (New Module) / çˆ¬è™«æ¨¡å—                â”‚
â”‚                                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  CrawlOrchestrator / çˆ¬å–ç¼–æ’å™¨                  â”‚   â”‚
â”‚  â”‚  - Coordinates crawl strategy                    â”‚   â”‚
â”‚  â”‚  - Manages state persistence                     â”‚   â”‚
â”‚  â”‚  - Generates reports                             â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚           â”‚                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  SitemapManager   â”‚  â”‚  LinkScout   â”‚  â”‚  State  â”‚ â”‚
â”‚  â”‚  - sitemap.xml    â”‚  â”‚  - Discover  â”‚  â”‚  - Save â”‚ â”‚
â”‚  â”‚  - discovery      â”‚  â”‚  - Filter    â”‚  â”‚  - Load â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  CrawlStrategy (Abstract Base) / çˆ¬å–ç­–ç•¥åŸºç±»   â”‚    â”‚
â”‚  â”‚  - define_link_priorities()                     â”‚    â”‚
â”‚  â”‚  - should_follow_link()                         â”‚    â”‚
â”‚  â”‚  - extract_metadata()                           â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚           â”‚                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ DefaultStrategy   â”‚ DocsStrategy â”‚ BlogStrategyâ”‚    â”‚
â”‚  â”‚                   â”‚              â”‚             â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Existing Modules (Minimal Changes)               â”‚
â”‚  - fetch_html() / HTMLè·å–                               â”‚
â”‚  - extract_internal_links() / å†…éƒ¨é“¾æ¥æå–                â”‚
â”‚  - process_pagination() / åˆ†é¡µå¤„ç†                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### File Structure / æ–‡ä»¶ç»“æ„

```
webfetcher/
â”œâ”€â”€ wf.py                          # Updated: expose all params
â”œâ”€â”€ webfetcher.py                  # Updated: add --follow-pagination flag
â”œâ”€â”€ crawler/                       # NEW: Crawler module
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ orchestrator.py            # CrawlOrchestrator
â”‚   â”œâ”€â”€ sitemap_manager.py         # Sitemap discovery and parsing
â”‚   â”œâ”€â”€ link_scout.py              # Link discovery and filtering
â”‚   â”œâ”€â”€ state_manager.py           # State persistence
â”‚   â”œâ”€â”€ strategies/                # Crawl strategies
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base.py                # Abstract base strategy
â”‚   â”‚   â”œâ”€â”€ default.py             # Default BFS strategy
â”‚   â”‚   â”œâ”€â”€ documentation.py       # Documentation site strategy
â”‚   â”‚   â”œâ”€â”€ blog.py                # Blog site strategy
â”‚   â”‚   â””â”€â”€ category_first.py     # Government site strategy (migrate)
â”‚   â””â”€â”€ reporters/                 # Output generators
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ json_reporter.py       # JSON index generation
â”‚       â”œâ”€â”€ csv_reporter.py        # CSV report generation
â”‚       â””â”€â”€ sitemap_reporter.py    # Mermaid diagram generation
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_crawler/              # NEW: Crawler tests
â”‚       â”œâ”€â”€ test_orchestrator.py
â”‚       â”œâ”€â”€ test_sitemap_manager.py
â”‚       â”œâ”€â”€ test_link_scout.py
â”‚       â”œâ”€â”€ test_state_manager.py
â”‚       â””â”€â”€ test_strategies.py
â””â”€â”€ docs/
    â””â”€â”€ crawler_guide.md           # NEW: Crawler documentation
```

---

## Implementation Plan / å®æ–½è®¡åˆ’

### Phase Breakdown / é˜¶æ®µåˆ†è§£

| Phase | Description | Effort | Dependencies |
|-------|-------------|--------|--------------|
| Phase 1 | Bug fixes + parameter exposure | 4-6h | None |
| Phase 2 | Sitemap support | 3-4h | Phase 1 |
| Phase 3 | Advanced crawling features | 4-6h | Phase 1 |
| Phase 4 | Structured output | 3-4h | Phase 1 |
| Phase 5 | Resume capability | 3-4h | Phase 1, 4 |

**Total Estimated Effort:** 14-19 hours (reduced from 16-22h due to robots.txt removal)
**Recommended Order:** 1 â†’ 2 â†’ 3 â†’ 4 â†’ 5

### Risk Mitigation / é£é™©ç¼“è§£

**Risk 1: Breaking existing crawl_site() callers / ç ´åç°æœ‰è°ƒç”¨è€…**
- Mitigation: Keep existing `crawl_site()` function signature
- Add new `crawl_site_enhanced()` with new features
- Gradually migrate over multiple releases

**Risk 2: State file corruption / çŠ¶æ€æ–‡ä»¶æŸå**
- Mitigation: Use JSON schema validation
- Write to temp file first, then atomic rename
- Keep backup of previous state

**Risk 3: Sitemap parsing failures / Sitemapè§£æå¤±è´¥**
- Mitigation: Robust error handling for malformed sitemaps
- Fall back to BFS crawling if sitemap parsing fails
- Support multiple sitemap formats (XML, gzipped, sitemap index)

---

## Testing Strategy / æµ‹è¯•ç­–ç•¥

### Unit Tests / å•å…ƒæµ‹è¯• (8-10 tests per module)

**Test crawler/sitemap_manager.py:**
```python
def test_sitemap_xml_discovery():
    """Test sitemap.xml discovery from /sitemap.xml and /sitemap_index.xml."""

def test_sitemap_url_extraction():
    """Test URL extraction from sitemap.xml."""

def test_sitemap_gzip_handling():
    """Test parsing of gzipped sitemaps (sitemap.xml.gz)."""

def test_sitemap_index_parsing():
    """Test parsing sitemap index files with multiple sitemaps."""
```

**Test crawler/link_scout.py:**
```python
def test_url_pattern_matching():
    """Test glob and regex pattern matching."""

def test_domain_boundary_enforcement():
    """Test same-domain filtering."""

def test_link_discovery_comprehensive():
    """Test link extraction from <a>, <link>, <iframe>."""
```

**Test crawler/state_manager.py:**
```python
def test_state_save_load():
    """Test state persistence and loading."""

def test_state_resume_correctness():
    """Test that resume doesn't duplicate visits."""

def test_state_corruption_handling():
    """Test handling of corrupted state files."""
```

### Integration Tests / é›†æˆæµ‹è¯• (5-7 scenarios)

**Test full crawl scenarios:**
```python
def test_wf_site_command_end_to_end():
    """Test wf site command from CLI to output."""
    # Run: wf site https://httpbin.org/html --max-pages 5
    # Verify: output files created, JSON index valid

def test_crawl_with_sitemap():
    """Test crawling using sitemap.xml."""
    # Setup: mock server with sitemap.xml
    # Run: wf site --use-sitemap
    # Verify: URLs from sitemap are crawled

def test_resume_interrupted_crawl():
    """Test resuming a partially completed crawl."""
    # Run: crawl 100 pages
    # Interrupt: after 50 pages
    # Resume: with --resume flag
    # Verify: no duplicate fetches, total = 100 pages
```

### Regression Tests / å›å½’æµ‹è¯•

- [ ] Add `wf site` test URLs to `tests/url_suite.txt`
- [ ] Run full regression suite after each phase
- [ ] Ensure backward compatibility with existing commands

---

## Documentation Requirements / æ–‡æ¡£è¦æ±‚

### User Documentation / ç”¨æˆ·æ–‡æ¡£

**1. Updated wf.py Help Text / æ›´æ–°å¸®åŠ©æ–‡æœ¬**
```bash
wf site --help
```
- Document all new flags and options
- Provide examples for common use cases
- Bilingual (English/Chinese)

**2. Crawler Guide / çˆ¬è™«æŒ‡å—** (`docs/crawler_guide.md`)
- Architecture overview
- Crawl strategy selection guide
- Sitemap.xml handling
- Resume capability usage
- Output format specifications
- Troubleshooting common issues
- Note: Personal use only, no robots.txt compliance

**3. Examples / ç¤ºä¾‹**
```bash
# Basic site crawl
wf site https://example.com

# Crawl with custom parameters
wf site https://docs.example.com --max-pages 200 --max-depth 3

# Crawl documentation site with pattern filtering
wf site https://example.com --include-pattern "/docs/*" --strategy documentation

# Resume interrupted crawl
wf site https://example.com --resume

# Generate JSON index and CSV report
wf site https://example.com --output-json index.json --output-csv report.csv
```

### Developer Documentation / å¼€å‘è€…æ–‡æ¡£

**1. Architecture Document / æ¶æ„æ–‡æ¡£**
- Component responsibilities
- Data flow diagrams
- Extension points for new strategies

**2. API Documentation / APIæ–‡æ¡£**
- Docstrings for all public functions
- Type hints for all function parameters
- Usage examples in docstrings

**3. Migration Guide / è¿ç§»æŒ‡å—**
- How to migrate from old `crawl_site()` to new enhanced crawler
- Breaking changes (if any)
- Deprecation timeline

---

## Acceptance Criteria / éªŒæ”¶æ ‡å‡†

### Functional Criteria / åŠŸèƒ½æ ‡å‡†

- âœ… **Bug Fix**: `wf site` command works without `--follow-pagination` error
- âœ… **Parameters**: All crawl parameters (depth, pages, delay) configurable via CLI
- âœ… **Sitemap**: Discovers and uses sitemap.xml when available
- âœ… **Pagination**: Follows pagination links correctly
- âœ… **URL Filtering**: Supports include/exclude patterns (glob and regex)
- âœ… **Domain Boundaries**: Enforces same-domain crawling by default
- âœ… **Strategies**: At least 3 crawl strategies implemented (default, docs, blog)
- âœ… **JSON Output**: Generates valid JSON index with complete metadata
- âœ… **CSV Output**: Generates importable CSV report
- âœ… **Resume**: Can resume interrupted crawls without duplicates
- âš ï¸ **Note**: No robots.txt compliance (personal use tool)

### Quality Criteria / è´¨é‡æ ‡å‡†

- âœ… **Test Coverage**: >85% code coverage for new crawler module
- âœ… **Regression**: All existing tests pass (30/30 or better)
- âœ… **Performance**: No performance regression (<10% slower than current)
- âœ… **Documentation**: Bilingual user guide and API docs complete
- âœ… **Backward Compatibility**: Existing `wf site` commands still work

### Performance Criteria / æ€§èƒ½æ ‡å‡†

- âœ… **Crawl Speed**: 5-10 pages/minute on average network
- âœ… **Memory Usage**: <500MB for 1000-page crawl
- âœ… **State File Size**: <1MB per 1000 URLs visited
- âœ… **Resume Overhead**: <5% extra time compared to fresh crawl

---

## Dependencies / ä¾èµ–

### Required Libraries / å¿…éœ€åº“

```python
# Standard library (already available)
import xml.etree.ElementTree  # For sitemap.xml parsing
import gzip  # For gzipped sitemap parsing
import json  # For state and JSON output
import csv  # For CSV output
import time  # For timestamps
import re  # For regex patterns
from pathlib import Path  # For file operations
```

**No new external dependencies required.** / æ— éœ€æ–°çš„å¤–éƒ¨ä¾èµ–ã€‚
**Note:** urllib.robotparser removed as robots.txt support was eliminated. / æ³¨æ„ï¼šå·²ç§»é™¤urllib.robotparserï¼Œå› ä¸ºä¸å†æ”¯æŒrobots.txtã€‚

### Related Tasks / ç›¸å…³ä»»åŠ¡

- **Task-007**: Dual-Method Regression Testing
  - New crawler should support dual-method testing
  - Extend `--dual-method` to site crawling mode

- **Task-002**: Regression Test Harness
  - Add crawler test scenarios to regression suite

- **Task-001**: Parser Template Creator
  - Crawled content should use template-based parsing

---

## Future Enhancements (Out of Scope) / æœªæ¥å¢å¼ºï¼ˆä¸åœ¨èŒƒå›´å†…ï¼‰

These features are explicitly **NOT** included in Task-008 but may be considered for future tasks:

1. **Distributed Crawling / åˆ†å¸ƒå¼çˆ¬å–:** Multi-machine crawling with shared state
2. **JavaScript Execution / JavaScriptæ‰§è¡Œ:** Full page rendering for all pages (too slow)
3. **Content Deduplication / å†…å®¹å»é‡:** Detect and skip duplicate content (different URLs, same content)
4. **Link Graph Analysis / é“¾æ¥å›¾åˆ†æ:** PageRank-style importance scoring
5. **Media Download / åª’ä½“ä¸‹è½½:** Automatic download of images, videos, PDFs
6. **Database Storage / æ•°æ®åº“å­˜å‚¨:** Store crawled pages in database instead of files
7. **Web UI / Webç•Œé¢:** Graphical interface for crawl configuration and monitoring

---

## Success Metrics / æˆåŠŸæŒ‡æ ‡

**Quantitative Metrics / é‡åŒ–æŒ‡æ ‡:**
- ğŸ¯ **Bug Resolution**: 100% (1/1 critical bug fixed)
- ğŸ¯ **Feature Completion**: 100% (all 5 phases complete)
- ğŸ¯ **Test Coverage**: >85% for new code
- ğŸ¯ **Regression Tests**: 100% pass rate (30/30 or better)
- ğŸ¯ **Documentation**: 100% bilingual coverage

**Qualitative Metrics / å®šæ€§æŒ‡æ ‡:**
- âœ¨ **User Experience**: CLI is intuitive and well-documented
- âœ¨ **Code Quality**: Clean architecture, well-tested, maintainable
- âœ¨ **Reliability**: Handles errors gracefully, state persistence works
- âœ¨ **Performance**: No noticeable slowdown from current implementation

---

## References / å‚è€ƒèµ„æ–™

### Related Files / ç›¸å…³æ–‡ä»¶

1. `wf.py:426-446` - Current `wf site` command handler
2. `webfetcher.py:3153-3360` - Current `crawl_site()` implementation
3. `webfetcher.py:3082-3152` - `crawl_site_by_categories()` (category-first strategy)
4. `webfetcher.py:2690-2720` - `process_pagination()` (pagination handling)
5. `tests/url_suite.txt` - Regression test URL suite

### External Standards / å¤–éƒ¨æ ‡å‡†

- [Sitemap Protocol](https://www.sitemaps.org/protocol.html)
- [Sitemap XML Format](https://www.sitemaps.org/protocol.html#xmlTagDefinitions)

---

## Notes / å¤‡æ³¨

**Design Philosophy / è®¾è®¡å“²å­¦:**
- âœ… **Backward Compatible**: Existing commands must continue to work
- âœ… **Progressive Enhancement**: New features are opt-in, not mandatory
- âœ… **Personal Use Tool**: Designed for personal/research use, not production crawling
- âœ… **Fail-Safe Defaults**: Safe defaults (rate limiting, same-domain crawling)
- âœ… **User Control**: Always provide override flags for customization
- âš ï¸ **No Robots.txt**: This tool does not respect robots.txt (personal use only)

**Implementation Notes / å®æ–½è¯´æ˜:**
- Phase 1 (bug fixes) should be completed first as it unblocks users
- Phases 2-5 can be developed in parallel by different developers
- Each phase should have its own git commit with detailed commit message
- All bilingual documentation must be verified for accuracy

---

**Created By / åˆ›å»ºè€…:** Architectural Analysis (Principal Architect)
**Last Updated / æœ€åæ›´æ–°:** 2025-10-10
**Status / çŠ¶æ€:** Ready for Review and Implementation / å¾…å®¡æŸ¥ä¸å®æ–½

---

ğŸ¤– Generated with [Claude Code](https://claude.com/claude-code)

Co-Authored-By: Claude <noreply@anthropic.com>
